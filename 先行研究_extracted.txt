Unsupervisedvisualsimilarity-basedmedicalimageretrievalviadual-encoderandmetriclearningXiyaWenga,1,YanZhuanga,1,RuiWangb,KeChena,*,LinHana,c,ZhanHuad,JiangliLina,*aCollegeofBiomedicalEngineering,SichuanUniversity,Chengdu610065,ChinabUltrasoundDepartmentattheGeneralHospitalofWesternTheaterCommand,Chengdu610083,ChinacHighongIntellimageMedicalTechnology(Tianjin)Co.,Ltd.,Tianjin300480,ChinadGeneralSurgeryDepartmentofChina-JapanFriendshipHospital,Beijing100028,ChinaARTICLEINFOKeywords:MedicalimageretrievalUnsupervisedlearningMetriclearningSkincancerdiagnosisVisualsimilarityABSTRACTSkincancerstandsasoneofthemostlethalcancers,withitsprognosisheavilyreliantupontimelydiagnosis.Medicalimageretrieval(MIR)techniquesaimtoretrieveimagessimilartothequeryonefromadataset,assistingdoctorsindiseasediagnosisandclinicaltreatmentprogramming.However,asignificantchallengeinderma-toscopicimageinterpretationstemsfromtheheterogeneityoflesionvisualappearance,whichhindersexistingmethodsfromeffectivelybalancingvisualsimilaritywithdiseasecategory.Toaddressthisissue,weproposeanunsupervisedapproachformedicalimageretrievaltoenhancethevisualcongruityofretrievalwhileensuringdiseaseaccuracy.Dual-Encoderistrainedtoextractimagefeaturesviaaself-distillingdualnetwork,andfollowedbysubsequentrefinementofthefeaturerepresentationutilizingunsupervisedmetriclearningmethodologies.OurmethodistestedontheskincancerdatasetISIC2019,evaluatedusingdual-dimensionalmetricsofvisualanddiseasesimilarity.Toassessvisualsimilarityperformance,weconstructedatestvisualdatasetandcategorizeinto27visualtypeswefine-tunedthepre-trainedmodelonmultipledatasetsencompassingultrasound,MRIandCTimagestoevaluateitsapplicability,whichoutperformsotheradvancedgeneralizedretrievalmethods.Theexperimentsdemostratedtheeffectivenessandgeneralizationofourproposedmethod.1.IntroductionCutaneousmalignancies,particularlymelanoma,poseaprevalentandchallenginghealthissueacrosstheglobe[1].Thepivotalelementinthetreatmentofskincanceristimelyscreening,astheearlydetectionofmelanomaiscorrelatingwithanenhancedfive-yearsurvivalrate,approaching90%[2].Medicalimagescontainwealthofpathologicalinsightsandplayacentralroleinclinicaldiagnosis,patientmonitoringandtherapeuticinterventions[3].Inthedomainofskincancerdiag-nosis,dermatologistscandiscernthebenignormalignantnatureofalesioninaqueryimagebyanalyzingCommonImagingSigns(CISs).Thesesignsincludeaspectrumofmorphologicalcharacteristicssuchaslesionshape,symmetry,boundarydefinition,aswellasvisualaspectslikecoloration,texture,pigmentationpatterns.Furthermore,clinicaldiagnosisofdermatologicalconditionsisconfrontedwithchallengesarisingfromthecomplexityofimagecharacteristics.Thephenomenaof"similarvisualexpressionmaskingdifferentpathologies"and"identicalpathologiesexhibitingvariantvisualexpressions",asshowninFig.1.Moreover,inthefaceofanexponentialincreaseofmedicaldigitalim-ages[4],thetaskofanalyzingandinterpretingextensivedatasetsposeasignificantchallengeforexpertclinicians,necessitatingsubstantialtimeandeffort.Variabilitybothbetweendifferentcliniciansandwithinthesameclinicianacrossdifferentinstancescanpotentiallyleadtodiag-nosticinconsistenciesinmedicalimagesassessment[5,6].MedicalImageRetrieval(MIR)presentsaviablesolutiontoenhancingdiagnosticaccuracyandefficiency.MIRfacilitatesthediscoveryofvisuallysimilarreferencecasesfrommedicaldatabases,therebyequippingphysicianstoenhancediagnosticprecisionandstreamlineclinicalworkflows[3,7].Additionally,MIRservesasaninvaluabletoolforeducationalandresearchendeavors,aidingtheidentificationofanalogouscasesforpedagogicalpurposes[8,9].Inrecentyears,therehasbeenanotableprogressioninthe*Correspondingauthors.E-mailaddresses:chenke@scu.edu.cn(K.Chen),linjiangli@scu.edu.cn(J.Lin).1Theseauthorscontributedtotheworkequallyandshouldberegardedasco-firstauthors.ContentslistsavailableatScienceDirectNeurocomputingjournalhomepelsevier.com/comhttps://doi.org/10.1016/j.neucom.2025.129861Received14October2024;Receivedinrevisedform11February2025;Accepted25February2025

developmentofautomatedretrievaltoolsfordermoscopicimages[1,10–12].ThearchitectureofcontemporaryMIRsystemsisdevidedintothreesequentialphasesencompassingpreprocessing,featureextraction,andsimilaritymetrics.Amongthese,theextractionofimagefeaturesholdsapivotalroleintheprocess[13].Conventionalmethodsrelyonexpertknowledgetocaptureidentifyandcapturethepertinentvisualattributesofimages.Theyarelimitedbytheinabilitytoautomaticallyadjustparametersandinabilitytoextractadequatevisualinformationfromimages[14].Thedeepconvolutionalneuralnetwork(DCNN)isawidelyadoptedmethodforfeaturerepresentationinMIR[3,5,13,15,16].ThestandardparadigminvolvesutilizingclassifierswithinaDCNN,employingcross-entropylosstoachieveclassification.Theensuingfeaturemaps,deprivedfromintermediatelayers,areutilizedforassessingimagesimilarity[7].Thisapproachemphasizesondeepse-manticsimilarityandenhancinginter-classdiscriminability,butitmaybeconstrainedbytheextentofavailableannotateddata.Itoverlooksintra-classdiscriminabilityandthespecificvisualrepresentation,fallingshortinprovisdingphysicianswithvisuallyanalogousretrievalresults.Anotherframeworkpredicatedontask-independenttrainingtocultivatearobustembeddingspacethatreinforcesthedistinctivenessofimagerepresentations.TheseframeworksoftrnutilizeSiamesenetworks,whichareweight-sharingnetworksforprocessingmultipleinputs.Notably,OzturkSetal.proposedanovelMIRmethodbasedonthetripletloss,OCAM(opponentclassadaptivemargin),demonstratingexcellentperformanceondermoscopyimages.Thislearningparadigmtranscendsamerefocusoncategoricaldisparities;itendeavorstocap-turediscerniblefeaturesthatdelineatebothinter-andintra-classimagevariations,therebyenhancingtheretrievabilityofimagesinmedicalcontexts.Thesuccessfulimplementationofdeepneuralnetworkreliesonac-cesstoextensivelabeledtrainingdata[17].However,theavailabilityofsubstantialmedicalimagedatasetsisdifficultduetohighannotationcostsandinconsistentqualityoflabels.Itlimitstheapplicationofsu-pervisedmethodsinMIR.Conversely,unsupervisedlearning[14,17–19]methods,whichderivefeaturesfromdatawithoutmanuallyannotations,haveseenanincreaseddemandinMIR.However,thelackofexplicitinformationguidanceinunsupervisedlearningcanleadthattheextractedfeaturesarenotadequatelycorrespondtothevisualcon-tentoftheimageormaycontainredundantinformation,potentiallyresultinginthetrainedmodelexhibituniformperformancevariedim-ages[17].Ourobjectiveistoinvestigateanunsupervisedretrievalframeworktosearchimageswithvisualsimilarities,focusingonhar-nessingthecomprehensiveinformationinherentintheimagesanddelineatingintra-classvariations.Inthisstudy,weproposedanovelunsupervisedframeworkthatintegratesSiamesenetworksandmetriclearningtechniquestogeneratefeaturedescriptorsforimageretrievaltasks.Asiamese-networkisemployedtotrainanencoder,followedbyafullyconnectedlayerwiththeReLUactivationtorefinefeatureselection.Inaddition,aquantization-basedunsupervisedmetriclearningstrategyisimple-mentedtopreservetheoriginalfeatureinformationwhileminimizingquantizationerror.Theprincipalcontributionsofthisworkareasfollows:Weproposedanunsupervisedframeworkformedicalimageretrieval,employingatwo-stagetrainingstrategytomaximizethesimilaritybetweenrelevantdepthdescriptors.Byutilizingunannotatedmedicaldatasets,weharnessasiamesenetworkencoderthatoperatesonaself-distillationmethodtoextractimagefeatures.Subsequently,anunsupervisedmetriclearningtechniqueisintro-ducedtorefinetherepresentationbyeliminatingredundantinforma-tion.Ourapproachenhancessensitivitytointra-classvariations,therebyimprovingvisualsimilarityperformance.Toevaluateretrievalperformance,were-labeledasubsetofimagesfromthepublicdatasetISIC2019,centeredonvisualsimilarity.Followingmeticulousclassificationandscreening,theresultingdatasetcomprised2833imageswith27distincttypesofvisualfeatures.ComprehensiveexperimentsconductedontheISIC2019datasetshowthatourapproachachievesuperioraccuracytootherimageFig.1.Illustrationsofmelanomadermoscopicimageswith"similarvisualexpression,differentdiseasetype"and"samediseasetype,differentvisualexpression"(greenbox:benign,redbox:malignant).X.Wengetal.

retrievalmethods.Furthermore,byfine-tuningthepre-trainedmodelonthyroidtumorultrasoundimages,brainMRIandliverCTdatasets,itdemonstraesttheefficacyoftheframeworkinmigratingtodifferenttypesofmedicalimages.2.RelatedworksDermoscopyisapopularnon-invasiveexaminationforskinlesionsusingadeviceequippedwithahigh-qualitymagnifyinglensandapolarizedilluminationsystem.Ithasbeenthesubjectofextensiveresearch[20–22]forlesiondiagnosisbasedonartificialintelligence(AI)usingdermoscopicimages.ZengQetal.[23]proposedasemi-supervisedmedicalimageclassificationmethodtermedPEFAT(Pseudo-lossEstimationandFeatureAdversarialTraining),demon-stratingpromisingperformanceondermoscopydatasets.MenziesSetal.[24]evaluatedthepracticalclinicalapplicationofAI,illustratingcomparablediagnosticaccuracybetweenAIsystemsanddermatologistsinthecontextofskincancer.ItsuggeststhatautomaticMIRsystems[1,7]fordermoscopydiagnosisprovidephysiciansavaluabletoolbypresentingsimilarcasestofacilitatediagnosticandtherapeuticdecisions.Incontent-basedMIR,imagesarerepresentedintoalatentfeaturespaceandretrievalisexecutedbycalculatingthesimilaritybetweenqueryimageandcandidateonesinthisspace[25].Theprocessoffeatureextractionisofparticularimportance,asitdirectlyimpactstheretrievaloutcome.Conventionalhandcraftedfeatures[26–28]areun-abletoadapttothevaryingmodalitiesofmedicalimages,thusfailingtocomprehensivelycharacterizeimagedistribution.RecentadvancementindeeplearningtechniqueshavepropelledthedevelopmentofMIRmethods[1].Intheclassifier-guidedframework,DeepConvolutionalNeuralNetwork(DCNN)aretrainedutilizingclassificationloss.Thenthefeaturedescriptorsextractedbeforetheclassificationlayerareemployedtocalculateimagesimilarityforretrievalprocess.Alterna-tively,DCNNscanbetrainedusingmetriclossortripletlossfunctionstoachieveimageembeddings.Itensuresthatimageswithhighvisualresemblancearemappedcloselywithintheembeddingspace,whereasthosethatarevisuallydissimilarhavedistantembeddingvectors[7,29].FangJiashengetal.[5]proposedanAttention-basedTripletHashingnetworkformedicalimageretrieval.Experimentsontwocase-basedmedicaldatasetshavedemonstratedthatthetripletcross-entropylossenhancestheclassificationperformanceanddiscriminabilityofhashcodes.OZTURKSetal.[7]proposedatriplet-learningmethodforautomatedmedicalimageretrieval,leveraginganOpponentClassAdaptiveMargin(OCAM)loss.OCAMconsidersrelationshipsamongallimagepairswithinthetripletanddynamicallyadjustsadaptivemarginvaluespecifictoeachdatasetthroughoutthetrainingprocess.Zhangetal.[29]proposedamethodwiththeSiameseneuralnetwork,forcomputer-aideddiagnosisoflungcancerandtuberculosisfromCTim-ages.Howeversupervisedmethodsrelyheavilyonlabels.thatareexclusivelyprovidedbyexpertphysicians,whichiscostlyandtime-consuming.Tomitigatethechallengeoflimitedlabeledmedicalimagedata,transferlearningstandsasaprevalentsolution[4,17,30].Forinstance,ShinHCetal.[31]demonstratedtheefficacyofpre-trainingonImage-Netandthenfine-tuningthemodelforimageretrievaltaskswithinthecontextoftheInterstitialLungDiseasedatasetandtheThor-acoabdominalLymphNodedataset.However,medicalimagingen-compassesspecificimagingmodalitiesandapplicationcontexts,thatsetitapartfromnaturalimages.Weaklysupervisedmethodologies[32],whichleveragepartiallyorimpreciselyannotateddata,havealsobeenusedtomitigatethediffi-cultyinobtainingsufficientlabeleddata.BIJetal.[33]demonstratedimprovedretrievalresultsonaclinicalgastroscopyimagedatasetbyutilizingagraph-basedsemi-supervisedlearningtechnique.HASHI-MOTONetal.[34]proposedacase-basedSimilarImageRetrieval(SIR)methodwithanattention-basedmulti-instancelearningstategytoextractinformativepatchesfromweaklyannotatedwholeslideimages.ZHUANGYetal.[35]proposedtheWeaklySupervisedSimilarityAnalysisNetwork(WSAN),designedtoenhancesimilarimageretrievedwithinalungCTdataset.CHENFetal.[36]proposedafine-grainedweakly-supervisedprogressivefeatureextractionapproach,furtherrefinedbyanattentionmechanismmoduletoenhancetheinitialtargetinformation.Unsupervisedmethodslearnfeaturerepresentationsfromunlabeleddata,avoidingthecostlyandlabor-intensivetaskofdataannotation.Thisapproachaimstouncovertheunderlyingstructureofdatabyoptimizingtheintrinsicrelationshipswithinthedataset.Recentstudies[18,19,37]inunsupervisedrepresentationlearninghaveshownprom-isingresults.LISRetal.[38]trainedanattention-basedhashencoderinaself-supervisedmannerforretrievinghistopathologywholeslideim-ages.Auto-encoders[39]hasdemonstratedeffectivenessinmedicalimageretrievaltasks.Forinstance,anunsupervisedmedicalimageretrievalmethodbasedonVariationalAutoEncoders(VAEs)proposedbyALVESCetal.[12].obtainedvisuallycoherentresultstothequeryimageontheISIC2019dataset.Additionally,¨OztürkS¸[4]proposedastackedautoencoderbasedondeepfeaturelabelingtogenerateretrievalcodesforX-Rayimages.Unsupervisedmethodsbasedonmetricloss[40]ofteninvolvessiamesenetworkarchitectures,asseeninapproacheslikeInstDisc[41],MoCo[18](MomentumContrast),andSimCLR[42](asimpleframeworkforcontrastivelearningofvisualrepresentations).Notably,DEHAENEOetal.[43]employedtheMoCofortrainingfeatureextractorsonhistologicalimages.LIBetal.[44]usedSimCLR[42]toacquireanddevelopfeaturesfromhistologicalimages.ALIZADEHSM[45]etal.usedthesiamesedeephashmodelforhistopathologyimageretrieval.TheSwAV[46]frameworkrepresentsaself-supervisedlearningparadigmthatintroducedclusteringalgorithmstogeneratepseudo-labels.QIYetal.[14]proposedanunsuperviseddeephashingretrievalmethodwithjointlossfunctioninwhichthefeaturedistribu-tionisevaluatedthroughclustering-derivedlabels.Unsupervisedmethodsemployingsiamesenetworkslearnrepresentationsbyopti-mizingthesimilaritybetweeninstances,asexemplifiedbySimSiam[47](SimpleSiameseRepresentationLearning)andBYOL[37](BootstrapYourOwnLatent).WICKSTROMKKetal.[48]proposedanunsuper-visedmethodforCTliverimageretrievalbasedonSimSiam[47],achievingcommendableretrievalperformance.Insummary,acontent-basedskinlesionretrievalsystemisdesignedtoidentifysimilarimageswithverifiedpathologicaldiagnosesforaqueryimage.Networksrelyingonlabelinformationarelesssensitivetointra-classvariationsandthedifficultyinannotationshasrestrictedtheapplicationofsupervisednetworks.Conversely,unsupervisedmethodshavesignificantadvantagesincopingwithretrievaltasksacrossdifferentdatasetsbyexploitingvisualfeaturesandtheunderlyingdatastructure.Inthisstudy,weproposdeanunsupervisedapproachthatleveragessiamesestructureandmetriclearningformedicalimageretrieval,whichfocusesmorecomprehensivelyonimagevisualfeatures.3.Methods3.1.OverallframeworkTheentireframeworkoftheproposedmedicalimageretrievalmethodisshowninFig.2andthedetailedprocessisshowninAlgorithm1.Itcontainstwoprimarystagesincludingfeatureextractionandretrieval.Theinitialstageincludesatwo-steptraining.Firstly,theSimSiamalgorithmisintroducedtoextractdiscriminativedeepfeaturesforimagesusingaSiameseneuralnetwork,therebytrainingthefeatureencoder.Multi-viewunsupervisedlearningstrategyisappliedinthesubsequentstep,whichsamplesmultipledifferentenhancementsoftheimagetooptimizethecodingnetworkandembeddinglayer.ThemetriclossfunctionFastAPbasedonquantizationapproximationisutilizedtomaphigh-dimensionalfeaturevectorsintoalow-dimensionalspace,X.Wengetal.

generatingacodinglibrarythatcontainsadescriptivefeaturevectorforeachimage.Intheonlinephase,retrievalresultsaregeneratedbycomputingandrankingtheEuclideandistancesbetweentheencodingofthequeryimageandthevectorswithinthelibrary.3.2.UnsupervisedencodingwithtwinnetworksWeemploytheself-distillationunsupervisedalgorithmbasedonaSiameseneuralnetworktotraintheimagefeatureencoder.Theback-bonenetworkistheResNet50[49].Givenanimagex,tworandomlyaugmentedviews,denotedasx(1)andx(2),aregenerated.Theseviewsareprocessedbytheencoderfwithshared-weightthatcomprisesthebackbonenetworkandaprojectionhead,Theimagefeaturerepresen-tationsaredenotedasz(1)=h(f(x(1)andz(2)=h(f(x(2).Subse-quently,z(1)andz(2)aresubjectedtoapredictionheadtoyieldp(1)=g(h(f(x(1))))andp(2)=g(h(f(x(2)))).Finally,thegoalistomaximizethecosinesimilaritybetweenp(1)(orp(2))andz(2)(orz(1)).ThesimilarityisrepresentedasinEq.1:Dp(1),z(2))=p(1)/‖p(1)‖2)⋅z(2)/‖p(2)‖2)(1)Where‖•‖2denotesthel2–norm.SimSiamalgorithmmaximizesthesimilaritybetweendifferentviewsofanimage,successfullyavoidingmodelcollapsewithoutnegativesamplepairsormomentumencoders.Apivotalcomponentofthisalgorithmisthestop-gradientoperation,whichtreatz(1)andz(2)asconstantswhencalculatingthelossfunction.ThesymmetriclossfunctionisdefinedasEq.2:L=Dp(1),stopgradz(2)))+Dp(2),stopgradz(1)))2(2)Fig.2.Structureofunsupervisedmedicalimageretrieval.X.Wengetal.

Algorithm1.FeatureDatabaseConstructionforImageRetrieval3.3.DimensionalityreductionviaunsupervisedmetriclearningDirectutilizationofrawdeepfeaturevectorsobtainedfromtheencoderinphaseoneforretrievalmightbeinefficient.Thus,it’snecessarytorefinefeaturesandremoveredundancy.Duringthetrainingstageoftheembeddingencodinglayer,positivesamplesaregeneratedbyimageaugmentation.ThelossmetricfunctionisFastAP[50]whichsortssamplesdiscretelyandoptimizestheprecision-recallcurvetomaximizetheareaunderthecurveduringthetrainingprocesstoenhanceretrievalaccuracy.ThedefinitionisshownasEq.3:FastAP∑zZFzRPRPzR/Fz(3)wherezrepresentsthedistancebetweenthequeryimageandimagesinthedataset,Zisarandomvariablerelatedtodistancez,Rdenotesthesetofneighboringobjectsintheretrievaltask,Pindicatestheprobabilityofanevent,thedistancedistributionofRisdenotedasPzR,andFPZ<zrepresentsthecumulativedistributionfunction.3.4.RetrievalWemeasuredsimilaritybetweenaparticularqueryimageencodingxiandtheencodingxjintheretrievallibraryusingEuclideandistance.Highersimilarityisindicatedbyalowerdistancevalue.Wedefinexiandxjasn-dimensionalvectorsinthefeaturespaceRn,andxix1i,x2i,…,xniT,xjx1j,x2j,…,xnjT,theMinkowskidistancebetweenthemisformalizedinformula(4):Lpxi,xj(∑ni1⃒⃒⃒xlixlj⃒⃒⃒p)1/p,p1(4)X.Wengetal.

pistheorderofthenorm,whichissetto1fortheManhattandis-tance.Whenp=2,Lp,becomestheEuclideandistance.ManhattandistanceandEuclideandistanceareextensivelyappliedinmedicalimageretrieval.Inthisstudy,weemploytheEuclideandistanceforassessingthesimilaritybetweenfeatureencodingsintheretrievalphase.Theretrievalprocessinvolvescalculatingdistancestorankthesimi-larityofimages,withthehighest-rankingimagesidentifiedasresults.3.5.EvaluationmetricsWeevaluatetheretrievalperformanceintermsoftwodimensions,diseasetypelabelsandvisualsimilarity.Inthissection,wedelineatetheevaluationmetricsemployed.Forthevisualassessment,weutilizedanovelvisuallabelingdataset,specificallycuratedtofacilitatetheeval-uationofvisualsimilaritywithinretrievedimages.Fiveevaluationmetricsareadoptedtomeasuretheprecisionandretrievalqualityinourexperiments.Precisionatk(p@k)measurestheproportionofrelevantresultsamongthetopkretrievedimages,asshowninEq.(5):p@k=(1⎬N)∑Ni=1Rki⎬k(5)WhereNisthetotalnumberofquerysamples,kisthetotalnumberofimagesintheresult,andRkiisthenumberofimageswithsamelabelasthequeryinthetopkretrievedresults.TheAverageprecision(AP)isdefinedasshowninEq.(6).APav-eragestherankpositionsofimagessimilartothequeryimagetomea-suretherankquality.AP=(1⎬R)∑Kk=1(Rk⎬k)rel(k)(6)whereKrepresentsthetotalnumberofretrievedimagesandRkreferstothecountofsame-classsamplesinthetopkretrievedresults.Meanaverageprecision(mAP)iscalculatedasthemeanoftheAPacrossallsamplesandisdefinedinEq.(7):mAP=(1⎬N)∑Ni=1APi(7)whererel(k)isanindicatorfunctionthatis1ifthek-thsampleisrele-vantand0otherwise.TheRecall@kmetricemphasizestheretrievalsystem’scapabilitytoreturnusefulinformationwithoutdemandingexhaustiveretrievalofallrelevantitems.It’scalculatedbyEq.(8):Reall@k=(1⎬N)∑Ni=1·(qi⎭TOP[1:k])(8)φ(•)istheindicatorfunction,whichoutputs1ifthereisatleastoneiteminthetopkretrievedresults(TOP[1:k])thathasthesamelabelasthelabelofthequeryqi,and0otherwise.F1scorecanbecomputedtoevaluatethebalancebetweenprecisionandrecall:F1=(2•Precision∗Recall)⎬(Precision+Recall)(9)Comparedwithrecall@k,mMV@k(majorityvoteatthetopksearchresults)isamorestrictmetricasmMV@krepresentswhetherthemajorityoftheretrievedresultsmatchthecategoryofthequerysample.It’scalculatedbyEq.(10):mMV@k=(1/N)∑Ni=1×(qi⎭MV[1:k])(10)WhereMV[1:k]returnsthepredictedlabelbythemajorityvoteamongthetop-kretrievedresults.δ(•)isadiscriminantfunctiontojudgeequalityasqiandMV[1:k].Itisinsufficienttoemphasizeclass-wiseretrievalperformanceexclusivelybecauseoftheconsiderableintra-classvariabilityinherentinmedicalimages.WhilevisualsimilarityiscrucialinMIR,thereisnoeffectiveandstandardmethodfortheevaluationofvisualsimilarity.Therefore,weevaluatethevisualeffectivenessofretrievalresultsbasedonvisuallylabeleddata.Furthermore,sampleswererandomlyselectedfromthetestsettocheckthetop10retrievalresults.4.Experiments4.1.DatasetsTodemonstratetheeffectivenessoftheproposedmethod,wecon-ductedexperimentsonfourdifferentdatasetsincludingISIC2019,BraTS,LiTS17andthyroidultrasoundimages.TheutilizationofthefourdatasetsintheexperimentisdelineatedinTable1andthedetailedin-formationofeachdatasetisdescribedbelow.TheInternationalSkinImagingCollaboration2019Challenge(ISIC2019)datasetconsistsofeighttypesofpathologicalconditions,includingactinickeratosis(AK),basalcellcarcinoma(BCC),benignkeratosis(BKL),dermatofibroma(DF),nevus(NV),melanoma(MEL),vascularskinlesions(VASC),andsquamouscellcarcinoma(SCC).Inthisstudy,weemployedatwo-phasetrainingstrategyforthemodeldevel-opment.Thefirsttrainingstageinvolves12254imagesfromtheISIC2019dataset,whichalsoserveastheretrievalset.Thesecondstageuses2126trainingimagesand521validationimages.ThethyroidtumorultrasoundimagesiscollectedfromWestChinaHospitalofSichuanUniversity.Weusedcroppedimagesofthetumorregioncomprising3911images.TheBraTSdatasetcontainsmultimodalMRIbrainimagedata,encompassingfourimagingmodalities:T1-weighted,T1-weightedpost-Gdcontrast-enhanced,T2-weighted,andT2-weightedFLAIR.Eachsampleisaccompaniedbyacorrespondingsegmentationmap.Inourexperiments,weutilizedthesegmentedimagesofthelesionregions.LiTS17comprisescontrast-enhancedCTliverimagesfrom7medicalcenters,with130CTscansinthetrainingsetandthe70inthetestset,predominantlyfrompatientswithcancerandmetastaticliverdisease.Wesampledslicesfromeachvolumeatintervalsof8slicesinregionswithoutliverslicesandatintervalsof5slicesinregionswithliverslices.Thedatasetwaspartitionedinto95volumesfortraining,14forvali-dation,resultinginafinaldatasetof5735trainingimages,1102vali-dationimages,and1055testimages.Inthestageoftrainingencoderandfine-tuningtheembeddedlayer,positivesamplesaregeneratedbyaugmentingtheoriginalimages.Priorresearch,suchasthatbysimCLR[42],hasshownthatthedataaugmentationtechniquesinfluencesthenetwork’slearningeffective-ness,withmorecomplexaugmentationbingbeneficialfortraining.Inthisstudy,varioustransformationsareappliedtoenhancetheimagessuchasrandomcropping,resizing,brightnessadjustment,andhori-zontalflipping.Theseaugmentationsenhancethedistinctivenessofimagedescriptors,leadingtoimprovedretrievalscores.4.2.ImplementationdetailsTheexperimentswereconductedbasedonthepytorchframeworkusingPython3.7onaGPUserver(GraphicsDevicewith12GBgraphicscard).TheconvolutionallayerswithintheResNet50architecture,alongTable1Datasets.DatasetTrainingsetValidationsetTestsetISIC20191225440854085TYROID2346782782BraTS1382461461LiTS17573511021055X.Wengetal.

withtheprojectionandpredictionheadoftheSiamesenetwork,werestreamlinedbyreducingtheirchannelcountsbyhalf.Theoutputdi-mensionsofthefinalprojectionheadweresetto1024.Duringthetrainingphaseoftheembeddingencodinglayer,eightenhancedver-sionsofeachoriginalimagewereobtained.TheStochasticGradientDescent(SGD)optimizerwasemployed,withinitiallearningratessetto0.01forthefirststageand0.001forthesubsequentstage.Amomentumvalueof0.9andaweightdecayof0.0005wereapplied.Thelearningratewasdynamicallyadjustedbasedontheepochcount,decrementingto20%ofitsoriginalvaluewhenthelossfunctionplateauedforfiveconsecutiveepochswithaminimumthresholdsetto1×108.Addi-tionally,trainingwasterminatedearlyifthelossremainedstagnantfortwelveconsecutiveepochs.4.3.Retrievalperformance4.3.1.PerformanceonbenchmarkdatasetWetrainedabaselineclassifierCNNguidedbydiseaselabelsontheISIC2019dataset.Thebackbonenetworkoftheclassificationmethodg(⋅)wasidenticaltothatofourencoderf(⋅),followedbyanadditionalfullyconnectedlayerwithReLUactivationasalinearclassifierg(⋅).Theclassifierwastrainedusingcross-entropyloss.Weconductedablationstudiestodemonstratetheeffectivenessofourmethod’scomponents.Toevaluatetheembeddingencodinglayer’sefficacyinfeaturesmapping,weperformedretrievalusingthe1024-dimensionalfeaturesgeneratedinthefirststageandcomparedtheaccuracybeforeandafterfeaturemapping.Inaddition,thenetworkistrainedsolelyusingtheunsuper-visedmetriclearningmethodofthesecondstageinordertoevaluatetheimpactofthefirstphaseoftraining.TheresultsarepresentedinTable2,withmAPshowninFig.3.Theresultsindicatethattheproposedun-supervisedmethodoutperformsmodelstrainedwithlabels,demon-stratingitsabilitytoeffectivelyleverageunlabeleddatatodifferentiatebetweenskincancerimages.Inaddition,itexceedstheotherablationmethods,withtop-1precisionachieving63.67%.ItreflectsthemodelperformingwellintermsoftheaccuracyanddominanceofcategoryretrievalThemMVis62.96%.Thisindicatesthatthecategorieswiththelargestnumberofretrievalresultsoftheproposedmethodaremostconsistentwiththequeryimage.Thisimprovementcanbeattributedtotheenhancedrepresentationlearningcapabilitiesofourfeatureextractor,whichallowsthemodeltobettercapturethesemanticsimi-laritiesbetweenthequeryandtheretrieveditems.TheretrievalresultsofdifferentimageretrievalmethodsbasedontheISIC2019datasetarepresentedinTable3.ThemAPoftheproposedapproachoutperformedthatofshallowIterativeQuantization(ITQ)Table2ResultsofforablationstudyonISIC2019.p@1p@5p@10R@5R@10F1scoremMVSimsiam(w/oembeddinglayer)0.60570.52020.49650.85560.92220.64550.5978MetricLearning(ML)0.62960.54570.52730.85560.92090.67060.6245CNNclassification(supervised)0.54780.55100.55400.79170.86600.67570.6156Proposed0.63670.55010.52450.86810.92340.66900.6296Fig.3.mAPscoresofdifferentretrievalmethodsonISIC2019.Table3RetrievalperformanceonISIC2019tootherretrievalmethods.p@1p@10R@5R@10F1scoremAPmMVITQ[51]0.37650.36440.69910.86960.51360.49940.3562TBH[52]0.51340.47780.82760.90640.62570.58360.5914SPQ[53]0.54290.51370.84700.92020.65930.62980.6218SimCLR[47]0.54190.47600.81760.89420.62130.59560.5658SwAV[46]0.60430.51350.85410.92210.66460.64220.6259BYOL[37]0.61370.51900.85810.92030.66370.65130.6272Proposed0.63670.52450.86810.92340.66900.66230.6296X.Wengetal.

method,deepsemi-unsupervisedTwin-BottleneckHashing(TBH),andSelf-supervisedProductQuantization(SPQ)by16.29%,0.22%,and3.25%.Comparedtothedeepsemi-unsupervisedmethodTBH,ourmethodexcelsbydiscerningcontentwithinmedicalimagesthroughthecomparisonofmultipleenhancedversionsoftrainingsamples,withoutrelianceonpriorinformationsuchaspretrainedmodelweights.OurmethodachievesbetterretrievalaccuracyontheskincancerdatasetthanSPQwithatrulyunsupervisedsetting.Specifically,weconsideredthesimilaritybetweendifferentviewsinanunsupervisedmannerandincorporatemetriclearningbasedonquantizationapproximationforindexencoding.Thecombinationofunsupervisedlearningandmetriclearningenhancesthedistinctivenessofdeepdescriptors.Moreover,ourapproachiscomparedwithotherstate-of-the-artun-supervisedcontrastivelearningmethodswithtwoparallelbranches,includingSimCLR[42],BYOL[37],andSwAV[46].TheSwAVlever-agesclusterassignmentpredictiontoproduceeffectivenegativesam-ples.BYOLenhancesthefeaturerepresentationthroughtheinteractionofanonlinenetwork.Thecomparisonexperimentusesthesamedis-tancemetricsasourmethod.SimCLRrequiresalargebatchsizeduringtrainingtoensuresufficientnegativesamplesforcontrastivelearning.Asaresult,itexhibitsrelativelylowerperformancewiththesamebatchsizesettingasothers.ThedetailedcomparisonresultsarepresentedinTable3.Theproposedmethodthanotherunsupervisedfeatureextrac-torsintermsofthefourmetricsontheISIC2019dataset.Specifically,ourmethodachievesimprovementsof2.3%inp@1comparedtothehighestvalueoftheothermethods.TheRecall@10valueis92.34%,indicatingthatthemodelcanbettercoverthesameimageasthequeryimagewell.TheincreaseinmAPwasrelativelyweak,generallyreachingthelevelofsuchparallelbranching-basedmethods.BycomprehensivelyanalyzingtheresultsofRecall,mAPandmMV,ourmethodshowsamorebalancedperformance.Resultsshowthataddingpositiveviewsfavorsnetworktrainingintheunsupervisedmethodsbasedonmetriclearning.TheresultofthesupervisedclassificationmodelisillustratedinFig.4wherethey-axisrepresentsthetruecategoryandthex-axisisthepredictedcategory.Theoverallclassificationaccuracyis0.6243,indi-catingacertainlevelofclassificationproficiency.However,whenuti-lizingfeaturespriortotheclassificationlayerforretrievaltasks,itfallsshortcomparedtotheunsupervisedmodel.Thisimpliesthatclassifi-cationaccuracydoesnotinherentlyguaranteeeffectiveretrievalper-formance,asimagessharingthesamepredictedlabelmaynotbecloselylocatedinfeaturespace.Incontrast,metriclearningismoresuitableforretrievaltasks,wherethespatialrelationshipisoffeaturesascrucialastheirdiscriminationpower.4.3.2.GeneralizationassessmentToassessthegeneralizationperformanceoftheproposedmethod,experimentswereconductedonacrossdiversedatasets,includingthy-roidtumorultrasound,colorMRIimagesandLiTS17datasets.Theprecision@kandmAPmetricsfromtheseimageretrievalexperimentsareshowninFigs.5and6.Weemployedamigrationlearningstrategybasedontheproposedmethodtoachievetheretrievaltaskonthesedatasets.Initially,thefeatureencoderispre-trainedontheISIC2019datasetandthenwetraintheembeddingencoderonthetargetdataset,followedbyacomprehensivefine-tuningoftheentirenetwork.Weperformedthefollowingablationexperiments.Firstly,thefeatureencoderwastrainedfromscratchdirectlyonthetargetdatasetFig.4.ConfusionmatrixforclassificationresultsofdiseaseclassificationmethodsonISIC2019.Fig.5.mAPscoresofdifferentretrievalmethodsonthyroidultrasoundimages,BraTSandLiTS17.Fig.6.Precision@Kscoresofdifferentretrievalmethodson(a)thyroidultrasoundimages,(b)BraTS,and(c)LiTS17.X.Wengetal.

(’abinitio’intable).Aitionally,weexploretheefficacyofe-scriptorsobtainefrofeatureencoeranMLlearningonlyfornetworktraining(’ML’intable),coparingthewithourapproach.AselineateinTable4,theresutsinicatethatourethoachievesu-periorprecision@kanAPof73.18%an77.85%onthyroiul-trasouniages.Theretrievalresultsshowsthatpre-trainingwithlargeratasetscanenhancefeaturelearninganretrievalprecision,especiallywhenthetargeteicalatasetisliite.TheresultsontheBraTSantheLiTS17showninTable4confirthatourproposeethoexhibitsiproveperforance.TheAPetricshowsa8.66%increaseoverthesuboptialMLethoontheriiages.Itshowsthattheoelcanbequicklyajustetootherifferenteicalatasetstobettereettheactualneesofiageretrieval.TheretrievaltaskontheLiTS17atasetisrelativelylessifficultincategoryiscriination,anourethoperforswelloneachetrics.Ai-tionally,ourethoachievesorethan99.0%inrecall@10,reflectingtheabilitytoreturniagesofrelatecategories.TheirscoresatMVwereet78.16%,76.79%an94.03%respectively.Theoelconsistentlyaintainssatisfactoryperforanceacrossiverseatasets,eonstratingrobustaaptabilityangeneralizationcapabilities.Thisenowstheoelwiththepotentialforiverseapplicationscenarios.Notably,weobservethattheiscriinativenessofebeingfortheoelirectlypre-traineonthetargetataseteclinesubsequenttofine-tuning.Thiscoulbeattributetothesizeoftargetataset,whichleatosuboptialoelinitialization,anthefine-tuninglearningrateinetriclearningistypicallylower.Insuary,ourtwo-stagestrategyiprovestheretrievalquality.Ablationexperientalresultseonstratesconsistentperforanceacrossultipleeicaliageatasets,exhibitingaegreeofgeneralizationcapability.TheretrievalresultsofifferentotheriageretrievalethosintheabovethreeatasetsarepresenteinTable5.Ourethohasachieveperforanceiproveentinseveralkeyetricsanshowngoocoprehensiveperforance.Theproposeethoexhibitsiprove-entsinAPonthethyroiultrasouniageatasetcoparewithhashingethos,outperforingtheITQ,TBHanSPQby8.97%,4.15%an7.29%,respectively.Ourethoachieves83.12%inF1-scorean78.16%inMV,bothofwhicharesuperiortoexistingethos.Theresultseonstratethecoprehensiveavantagesofourapproachinaressingthechallengesofiageretrievaltasks.OntheBraTSataset,althoughourethoinotachievethehighestRecallvalue,itattaineahigherF1-scoreof81.74%.ThisinicatesthatourethoeffectivelybalancesPrecisionanRecall.Ouretho’sp@1anMVvaluesare6.23%an3.33%higherthanthoseofthebest-perforingexistingethos,respectively.ThissuggeststhatourethoiscapableoforeaccuratelyretrievingiageswithseanticsiilarityontheMRIiages.OntheLiTS17ataset,theproposeethoachieve92.88%inAPan94.03%inMVforcategoryatching,bothofwhicharesuperiortoexistingethos.ConsieringPrecisionanotheretricsontheifferentatasets,theseresultsvaliatethegooaaptabilityoftheproposeethoacrossiverseatasetswithsoeajustents.4.4.Assessmentofvisualsimilarity4.4.1.CollectionswithvisualinsightsMeicaliagesoftenexhibitaconsierableintra-classvariation,Table4ResultsforablationstuyonthyroiUltrasouniages,BraTSanLiTS17.p@1p@10R@5R@10F1scoreMVthyroiAbinitio0.72800.68800.95270.98340.80960.7305Sisia0.70500.69050.94890.97960.81010.7229MetricLearning(ML)0.71390.72040.95660.98340.83160.7791Propose0.73180.71570.95910.99110.83120.7816BraTSAbinitio0.61390.58000.96310.99130.73190.6334Sisia0.62040.58890.95880.98910.73830.6616MetricLearning(ML)0.61610.60610.94580.98690.75100.6790Propose0.72020.69440.97180.99350.81740.7679LiTS17Abinitio0.86640.87930.94790.96970.92220.8957Sisia0.68910.66230.87770.94310.77810.6824MetricLearning(ML)0.90430.90240.98290.99520.94650.9317Propose0.90810.90840.98670.99620.95030.9403Table5ResultsofifferentretrievalethosonthyroiUltrasouniages,BraTSanLiTS17.p@1p@10R@5R@10F1scoreAPMVthyroiITQ[51]0.71010.60770.91060.99870.75560.68880.7050TBH[52]0.67430.67640.96420.99110.80400.73700.7330SPQ[53]0.54530.60090.94250.99620.74960.70560.6948SiCLR[47]0.70840.67860.95520.98470.80350.75540.7315SwAV[46]0.72940.70030.95640.97690.81580.77340.7346BYOL[37]0.72940.69900.94850.98450.81750.76760.7552Propose0.73180.71570.95910.99110.83120.77850.7816BraTSITQ[51]0.57920.51020.97610.99780.67520.61500.6377TBH[52]0.54880.55900.95440.98690.71380.65030.5922SPQ[53]0.60470.61300.95220.99130.75690.70300.6790SiCLR[47]0.63910.61610.95220.98700.75860.70330.7152SwAV[46]0.65210.58590.97830.99780.73830.69640.6739BYOL[37]0.65790.65960.97370.98900.79140.73350.7346Propose0.72020.69440.97180.99350.81740.77100.7679LiTS17ITQ[51]0.61150.58930.98160.99560.74040.68450.6375TBH[52]0.82700.82150.97150.98800.89710.86360.8511SPQ[53]0.83890.82180.97820.99430.89990.86740.8483SiCLR[47]0.80360.77290.92410.95830.85560.82310.8055SwAV[46]0.91350.89630.98000.98670.93930.92300.9230BYOL[37]0.89310.86330.96950.98380.91960.90170.8893Propose0.90810.90840.98670.99620.95030.92880.9403X.Wengetal.

implyingthatimageswithinthesameclasshavedisplayarangeofvisualappearances.InthecontextofMIR,thecriterionforretrievalextendsbeyondsimpleclassmembershipandit’sessentialthattheretrievedimagescloselyresemblethevisualcharacteristicsofthequeryimage.were-labeledasubsetofimagesfromtheISIC2019,whichwereoriginallyannotatedbasedondiseasecategories.Weintroducednewlabelsderivedfromvisualfeatures,aimingtocapturepatternsofvisualinformationthatmaynotalignwiththeoriginaldiseaselabels.Thepartialre-labelingenhancesthepracticalityofconductingevaluationsinretrievaltasksthatrequiresubtlevisualdistinctions.Forthevisualfeaturesatdifferentaspectsoftheimage,weperformagradualrefine-mentofthedistinction.Westartedwiththemostsalientandrecogniz-ablefeatureslikeshapeorcolor.There-labelinginitiativeincludedthreemedicalprofessionalsandonenon-expertparticipant.Theycollabora-tivelyidentifiedEasilydistinguishablevisualfeaturesandselectedrepresentativereferenceimages.Duringthejudgmentprocess,partici-pantswerepresentedwithimagestobeclassifiedandoptionalvisuallabelswiththeircorrespondingreferenceimages.Theimageswereclassifiedintorespectivecategorieswhenthefourparticipants’selectionwasconsistent.Subsequently,Buildinguponthedistinctivecharacter-isticsofeachcategory,weexecutedasecondaryclassificationfocusingonattributeslikesizeandotherpertinentcharacteristics.Thecatego-rizationprocessatthisstageismirroredthefirstlevel.Thefollow-upsubcategorizationisthesame,withdetailsshowninFig.7.Tobalancethedataset,weappliedmeasuressuchasrelaxedcriteria,allowingforsomeduplicatedimages,andmergingcertaincategories.Thefinaldatasetwithvisualfeaturecontains27classesand2833samples.4.4.2.MetricsforvisualconsistencyWeevaluatedthevisualsimilarityofthemodel’sretrievalresultsusingdatasetcuratedbasedonvisualfeature.TheretrievalaccuracyoftheablationexperimentsareshowedinTable6.Fig.8comparestheretrievalprecisionperformanceofthebaselinesupervisedCNNwithourmethodon12visualcategoriesamongthetotal.TheproposedapproachdemonstratesasignificantlyhigherproportionofsimilaritywiththequeryimagescomparedtothebaselineCNNclassificationmethod.Weconductedrandomchecksontheretrievalresultsforquerysamples.AsillustratedinFig.9,resultsobtainedfromproposedapproachexhibitamorepronouncedvisualresemblancetothequeryimagesthanthoseretrievedbytheclassifier-basedmethod.Additionally,themodelwithouttheembeddedencodinglayerdemonstratesarelativelyFig.7.Schematicrepresentationofdatasetconstruction.(a)Initialstepinvolvescategorizingdataintodistinctclassesbasedonuniquemorphologicalcharacter-istics,includingfeaturessuchas(i)reticulated,(ii)mottled,(iii)blue,and(iv)red,presentedfromlefttoright.(b)and(c)furthersubdividereticulatedandmottledfeaturesintosizeattributesandotherfeatures.Categorieswithinredandbluefeaturesaresubsequentlydividedintothosewithbrownedgesversusthosewithout,andwithwhitereticulationversuswithout,asillustratedin(d)and(e).Mottledandreticulatedimagesclassifiedbysizearefurthercategorizedbycolorshadesin(f),(g),(h),(i),(j),and(k).Table6RetrievalperformanceforablationstudyonISIC2019(basedonvisuallabel).p@1p@5p@10R@5R@10mAPSimsiam0.43950.34890.31240.68330.76010.4704MetricLearning(ML)0.47410.41340.38310.73120.80610.5199CNNclassification0.23800.22000.20520.52020.65450.3371Proposed0.48560.41150.38400.72550.80230.5244Fig.8.mAPscoresonISIC2019(12ofthesecategoriesbasedonvisuallabel).X.Wengetal.

consistentvisualpresentationinitsretrievals.Theseobservationssug-gestthattheunsupervisedtrainingofthenetwork’sencodereffectivelycapturesimagefeatures.Theyalsoimplythattheembeddedencodinglayerefficientlymapsthehigh-dimensionalfeaturesfromthefeatureencodertoalower-dimensionalspace,whichaidsineliminatingredundantinformation.Inthevisualevaluationmetrics,thestrictindi-catorofmMVwasnotemployedduetothecomplexityofvisualcate-gories.Table7presentsthecomparativeresultsofourmethodagainstexistingapproaches.Methodsbasedonpriorpre-trainedmodels,suchasITQandTBH,exhibitinferiordistinctivenessinvisualfeatureswithintheirretrievaloutcomes.Ourmethodachievesthebestvaluesinp@1(48.56%),andmAP(52.44%).Therecall@10hasimprovedby1.58%overtheotherbestmethods.Itindicatesthattheproposedapproachpossessesdiscriminativeperformanceintermsofvisualsimilaritydur-ingretrieval,therebydemonstratingitssuperiorityinoverallaccuracyandcomprehensiveness.TheretrievalresultsforaselectionofexamplesacrossdifferentmethodsisillustratedinFig.10.Theproposedmethodexhibitenhancedvisualconsistency.TheretrievalresultsontheBraTSandLiTS17data-setsareshowninFig.11.Theleftmostimagesserveasthequery,whiletheremainingimagestotherightshowcasethetop-1totop-10rankedretrievalsintermsoffeaturesimilarity.Theseresultsunderscorethemethod’sproficiencyinaligningwithvisualcoherenceacrossdiversedatasets.Tosubstantiatetheefficacyoftheproposedfeaturesinimprovingvisualsimilarity,weconductedavarianceanalysisofthedifferencesbetweenthequery-resultpairs,asdepictedinFig.12.Thisinvolvesdeterminingthedistancebetweenthequeryimageandthetop-fiveFig.9.Randomsamplequeryresultsusing(a)Unsupervisedretrievalmethod(b)modelwithoutembeddedcodinglayer(c)Diseaseclassificationmethod.Theleftmostimageineachcolumnisthequeryimage,andthetop1-top10retrievalresultsareshownontheright.Table7RetrievalperformanceofdifferentretrievalmethodsonISIC2019(basedonvisuallabel).p@1p@5p@10R@5R@10mAPITQ[51]0.11900.06140.06160.29370.44150.1993TBH[52]0.12090.11790.08850.24950.44150.1754SPQ[53]0.43560.40420.38570.71080.77950.5005SimCLR[47]0.33270.26650.24310.61920.71540.3946SwAV[46]0.45580.41350.39460.72500.78650.5133BYOL[37]0.43040.36460.33660.66960.74460.4701Proposed0.48560.41150.38400.72550.80230.5244X.Wengetal.

Fig.10.Randomsamplequeryresultsusing(a)transferlearningmethodbasedonunsupervisedretrievalframework(b)abinitiotrainingmethodbasedonun-supervisedretrievalframework(c)diseaseclassificationmethod.Theleftmostimageineachcolumnisthequeryimage,andtherightsideshowsthetop1-top10retrievalresults.Fig.11.Top-10retrievalresultsforsamplesfrom(a)BraTSand(b)LiTS17datasetsusingtheproposedmethods.Fig.12.ProbabilitydensitydistributionplotsofthevariancebetweenthesimilaritydistancesofthequeryresultsonthetestsetfromBraTS,LiTS17,ISIC2019andThyroidultrasoundimages.X.Wengetal.

retrievedimages,followedbycalculatingthevariance.ThevarianceoftheISIC2019datasetiscenteredbetween0.0002and0.015.FortheBraTSdataset,itvariesbetween0.00024and0.012.theLiTS17datasetfrom0.0035to0.016,andthethyroidultrasounddatasetshowsvaria-tionbetween0.00002and0.0018.Therefore,Thesefindingssuggestthatthefeaturesobtainedfromtheproposedretrievalmethodeffec-tivelypreservethevisualsimilaritybetweenimagesandthestabilityoftheresults.Furthermore,theproposedmethoddemonstratesarelativelylowtimecomplexityduringtheonlineretrievalphase,asindicatedinTable8,withsatisfactorytimeconsumptionunderidenticalexperi-mentalconditions.Specifically,Wemeasuredthetimerequiredintheonlinephase,whichincludesloadingthemodelandreturningthetop10mostsimilarimagesbycomparingwiththefeaturelibrarygeneratedintheofflinephase.Theresultsdemonstratethattheonlineretrievalofthetop10similarimagescanbeaccomplishedinmerely0.22seconds.Duringtheexperiments,weobservedthatthesizeofthefeaturelibraryhasacertainimpactontheretrievalspeed.However,sincethemedicaldatasetusedissignificantlysmallerthanlarge-scalenaturalimagedatasets,theefficiencyofmodelloadinghasamorepronouncedeffectonthetimeconsumption.Amongthemethodscompared,theprior-basedmodel,whichnecessitatesthesequentialloadingoftwomodelsforfeatureextraction,incursarelativelyhighertimecost.Underthecurrentdatascale,althoughtheSPQ(ProductQuantization)methodtheoreticallyholdscertainadvantages,theadditionaloverheadsasso-ciatedwithquantizationandsubvectoraccumulationdetractfromitsperformance,renderingitlessefficientthandirectEuclideandistancecomputation.Ourmethod,whichemploysthesamefeaturedimen-sionalityasotherdual-branchcontrastivelearningmethods,maintainscomparabletimecomplexityinretrievalandexhibitshighefficiency,makingitconducivetopracticalapplications.5.ConclusionWehaveproposedanunsupervisedmedicalimageretrievalapproachbasedonimagevisualsimilarity.TheapproachliesintheutilizationoftheunsupervisedtechniqueSimSiamandashallowdimensionreductionnetworktrainedviametriclearningmethodswithinthecontextofmedicalimageretrieval.Ourapproachhasbeentestedonskincancerdatasetsandthreeadditionalsmallermedicaldatasets.Theexperimentresultsonskincancerdatasetdemonstratethatourmethodachievesimpressiveresultsincapturingbothvisualanddisease-relatedsimilarities.Furthermore,theresultsacrossotherdata-setsunderscorethemodel’scapacityofgeneralization.Ourapproachreducesintra-classdisparitiesandincreasesinter-classdisparities.Itdemonstratesthepotentialofusinglargecollectionsofunlabeledmedicaldatatocharacterizemedicalimagefeatures.Atpresent,thevisualassessmentofmedicalimageretrievalresultsrequiresexpertconsultation,asvisualattentioncriteriamayvarywithdifferentmedicalimagestypes.Exploringhowtoassessvisualsimilarityandenhancingtheautonomyoftheseassessmentsinmedicalimageretrievalisasub-jectthatmeritsfutureresearch.CRediTauthorshipcontributionstatementLinJiangli:Resources,Projectadministration,Fundingacquisition,Supervision,Writing–review·editing.WengXiya:Writing–originaldraft,Validation,Methodology,Conceptualization,Formalanalysis.HanLin:Visualization,Validation,Formalanalysis.ChenKe:Writing–review·editing,Investigation,Fundingacquisition,Resources,Su-pervision.WangRui:Supervision,Validation,Writing–review·editing.ZhuangYan:Writing–review·editing,Supervision,Meth-odology,Fundingacquisition,Conceptualization,Formalanalysis,Projectadministration,Resources.HuaZhan:Datacuration,Investigation.DeclarationofCompetingInterestTheauthorsdeclarethattheyhavenoknowncompetingfinancialinterestsorpersonalrelationshipsthatcouldhaveappearedtoinfluencetheworkreportedinthispaperAcknowledgementsThisstudywassupportedbytheNationalNaturalScienceFounda-tionofChina(No.62406211)andtheNaturalScienceFoundationofSichuanProvince(No.2024NSFSC0654andNo.2023NSFSC0636).Also,wegratefullyacknowledgetheSichuanUniversityBiomedicalEngi-neeringExperimentalTeachingCenterfortheirinvaluablesupportandtheuseoftheiradvancedfacilities.DataavailabilityDatawillbemadeavailableonrequest.References[1]W.Barhoumi,A.Khelifa,Skinlesionimageretrievalusingtransferlearning-basedapproachforquery-drivendistancerecommendation,Comput.Biol.Med.137(2021)104825,https://doi.org/10.1016/j.compbiomed.2021.104825.[2]ZongyuanG.,DemyanovS.,ChakravortyR.,etal.SkinDiseaseRecognitionUsingDeepSaliencyFeaturesandMultimodalLearningofDermoscopyandClinicalImages[M].2017.https://doi.org/10.1007/978-3-319-66179-7_29.[3]Z.Li,X.Zhang,H.Muller,etal.,Large-scaleretrievalformedicalimageanalytics:acomprehensivereview,Med.ImageAnal.43(2018)66–84,https://doi.org/10.1016/j.media.2017.09.007.[4]S¸.¨Oztürk,Stackedauto-encoderbasedtaggingwithdeepfeaturesforcontent-basedmedicalimageretrieval,ExpertSyst.Appl.161(2020)113693,https://doi.org/10.1016/j.eswa.2020.113693.[5]J.Fang,H.Fu,J.Liu,Deeptriplethashingnetworkforcase-basedmedicalimageretrieval,Med.ImageAnal.69(2021)101981,https://doi.org/10.48550/arXiv.2101.12346.[6]R.Vishraj,S.Gupta,S.Singh,Acomprehensivereviewofcontent-basedimageretrievalsystemsusingdeeplearningandhand-craftedfeaturesinmedicalimaging:researchchallengesandfuturedirections,Comput.Electr.Eng.104(2022),https://doi.org/10.1016/j.compeleceng.2022.108450.[7]S.Ozturk,E.Celik,T.Cukur,Content-basedmedicalimageretrievalwithopponentclassadaptivemarginloss,Inf.Sci.637(2023),https://doi.org/10.1016/j.ins.2023.118938.[8]G.Hassan,K.M.Hosny,R.M.Farouk,etal.,AnEfficientretrievalsystemforbiomedicalimagesbasedonradialassociatedlaguerremoments,IEEEAccess8(2020)175669–175687,https://doi.org/10.1109/ACCESS.2020.3026452.[9]C.K.Chen,M.Y.Lu,D.F.K.Williamson,etal.,Fastandscalablesearchofwhole-slideimagesviaself-superviseddeeplearning(-+),Nat.Biomed.Eng.6(12)(2022)1420,https://doi.org/10.1038/s41551-022-00929-8.[10]HuB.,VasuB.,HoogsA.X-MIR:EXplainableMedicalImageRetrieval;proceedingsofthe2022IEEE/CVFWinterConferenceonApplicationsofComputerVision(WACV),F3-8Jan.2022,2022[C]https://doi.org/10.1109/WACV51458.2022.00161.[11]M.Goyal,T.Knackstedt,S.Yan,etal.,Artificialintelligence-basedimageclassificationmethodsfordiagnosisofskincancer:challengesandopportunities,Comput.Biol.Med.127(2020)104065,https://doi.org/10.1016/j.compbiomed.2020.104065.[12]AlvesC.,TrainaA.J.M.VariationalAutoencodersforMedicalImageRetrieval;proceedingsofthe2022InternationalConferenceonINnovationsinIntelligentSysTemsandApplications(INISTA),F8-12Aug.2022,2022[C].https://doi.org/10.1109/INISTA55318.2022.9894251.Table8TimeconsumptionofdifferentretrievalmethodsintheonlinestageonISIC2019.Timeconsumption(seconds)ML0.3164ITQ[51]0.3936TBH[52]0.3794SPQ[53]0.9034SwAV[46]0.2328SimCLR[47]0.3519BYOL[37]0.2380Proposed0.2251X.Wengetal.

[13]S.Ozturk,Class-drivencontent-basedmedicalimageretrievalusinghashcodesofdeepfeatures,Biomed.SignalProcess.Control68(2021),https://doi.org/10.1016/j.bspc.2021.102601.[14]Y.Qi,J.Gu,Y.Zhang,etal.,Unsuperviseddeephashingbyjointoptimizationforpulmonarynoduleimageretrieval,Measurement159(2020)107785,https://doi.org/10.1016/j.measurement.2020.107785.[15]C.G.Sotomayor,M.Mendoza,V.Castaneda,etal.,Content-basedmedicalimageretrievalandintelligentinteractivevisualbrowserformedicaleducation,researchandcare,Diagnostics11(8)(2021),https://doi.org/10.3390/diagnostics11081470.[16]J.Ahmad,K.Muhammad,S.W.Baik,Medicalimageretrievalwithcompactbinarycodesgeneratedinfrequencydomainusinghighlyreactiveconvolutionalfeatures,J.Med.Syst.42(2)(2018),https://doi.org/10.1007/s10916-017-0875-4.[17]E.Ahn,A.Kumar,M.Fulham,etal.,Convolutionalsparsekernelnetworkforunsupervisedmedicalimageanalysis,Med.ImageAnal.56(2019)140–151,https://doi.org/10.1016/j.media.2019.06.005.[18]HeK.,FanH.,WuY.,etal.MomentumContrastforUnsupervisedVisualRepresentationLearning;proceedingsofthe2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),F13-19June2020,2020[C].https://doi.org/10.48550/arXiv.1911.05722.[19]HeK.,ChenX.,XieS.,etal.MaskedAutoencodersAreScalableVisionLearners[M].2022.https://doi.org/10.48550/arXiv.2111.06377.[20]R.C.Maron,M.Weichenthal,J.S.Utikal,etal.,Systematicoutperformanceof112dermatologistsinmulticlassskincancerimageclassificationbyconvolutionalneuralnetworks,Eur.J.Cancer119(2019)57–65,https://doi.org/10.1016/j.ejca.2019.06.013.[21]H.A.Haenssle,C.Fink,F.Toberer,etal.,Managainstmachinereloaded:performanceofamarket-approvedconvolutionalneuralnetworkinclassifyingabroadspectrumofskinlesionsincomparisonwith96dermatologistsworkingunderlessartificialconditions,Ann.Oncol.31(1)(2020)137–143,https://doi.org/10.1016/j.annonc.2019.10.013.[22]P.Tschandl,C.Rosendahl,B.N.Akay,etal.,Expert-leveldiagnosisofnonpigmentedskincancerbycombinedconvolutionalneuralnetworks,JAMADermatol.155(1)(2019)58–65,https://doi.org/10.1001/jamadermatol.2018.4378.[23]ZengQ.,XieY.,LuZ.,etal.PEFAT:BoostingSemi-SupervisedMedicalImageClassificationviaPseudo-LossEstimationandFeatureAdversarialTraining;proceedingsofthe2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),F17-24June2023,2023[C].https://doi.org/10.1109/CVPR52729.2023.01504.[24]S.W.Menzies,C.Sinz,M.Menzies,etal.,Comparisonofhumansversusmobilephone-poweredartificialintelligenceforthediagnosisandmanagementofpigmentedskincancerinsecondarycare:amulticentre,prospective,diagnostic,clinicaltrial,LancetDigit.Health5(10)(2023)e679–e691,https://doi.org/10.1016/S2589-7500(23)00130-9.[25]X.Zhang,C.Bai,K.Kpalma,OMCBIR:offlinemobilecontent-basedimageretrievalwithlightweightCNNoptimization,Displays76(2023)102355,https://doi.org/10.1016/j.displa.2022.102355.[26]T.Sunitha,T.S.Sivarani,NovelcontentbasedmedicalimageretrievalbasedonBoVWclassificationmethod,Biomed.SignalProcess.Control77(2022),https://doi.org/10.1016/j.bspc.2022.103678.[27]P.Shamna,V.K.Govindan,K.Nazeer,A.Content-basedmedicalimageretrievalbyspatialmatchingofvisualwords,J.KingSaud.Univ.-Comput.Inf.Sci.34(2)(2022)58–71,https://doi.org/10.1016/j.jksuci.2018.10.002.[28]A.Khatami,M.Babaie,A.Khosravi,etal.,Paralleldeepsolutionsforimageretrievalfromimbalancedmedicalimagingarchives,Appl.Softw.Comput.63(2018)197–205,https://doi.org/10.1016/j.asoc.2017.11.024.[29]K.Zhang,S.Qi,J.Cai,etal.,Content-basedimageretrievalwithaconvolutionalsiameseneuralnetwork:distinguishinglungcancerandtuberculosisinCTimages,Comput.Biol.Med.140(2022),https://doi.org/10.1016/j.compbiomed.2021.105096.[30]N.Hegde,J.D.Hipp,Y.Liu,etal.,Similarimagesearchforhistopathology:SMILY[J],NPJDigit.Med.2(2019),https://doi.org/10.1038/s41746-019-0131-z.[31]H.-C.Shin,H.R.Roth,M.Gao,etal.,Deepconvolutionalneuralnetworksforcomputer-aideddetection:CNNarchitectures,datasetcharacteristicsandtransferlearning,IEEETrans.Med.Imaging35(5)(2016)1285–1298,https://doi.org/10.1109/TMI.2016.2528162.[32]YanX.,ZhangL.,LiW.-J.Semi-superviseddeephashingwithabipartitegraph;proceedingsofthe26thInternationalJointConferenceonArtificialIntelligence,IJCAI2017,August19,2017-August25,2017,Melbourne,VIC,Australia,F,2017[C].InternationalJointConferencesonArtificialIntelligence.https://doi.org/10.24963/ijcai.2017/452.[33]BiJ.,YinS.ANewGraphSemi-SupervisedLearningMethodforMedicalImageAutomaticAnnotation;proceedingsofthe2018IEEEInternationalConferenceonInternetofThings(iThings)andIEEEGreenComputingandCommunications(GreenCom)andIEEECyber,PhysicalandSocialComputing(CPSCom)andIEEESmartData(SmartData),F30July-3Aug.2018,2018[C].https://doi.org/10.1109/Cybermatics_2018.2018.00041.[34]N.Hashimoto,Y.Takagi,H.Masuda,etal.,Case-basedsimilarimageretrievalforweaklyannotatedlargehistopathologicalimagesofmalignantlymphomausingdeepmetriclearning,Med.ImageAnal.85(2023),https://doi.org/10.1016/j.media.2023.102752.[35]Y.Zhuang,N.Jiang,S.Chen,WSAN:aneffectivemodelofweaklysupervisedsimilarityanalysisnetworkforthelungCTimages[J],IEEEAccess10(2022)53777–53787,https://doi.org/10.1109/ACCESS.2022.3174099.[36]ChenF.,YouL.,ZhaoW.,etal.Centralizedcontrastivelosswithweaklysupervisedprogressivefeatureextractionforfine-grainedcommonthoraxdiseaseretrievalinchestx-ray[J].2022.https://doi.org/10.1002/mp.16144.[37]GrillJ.-B.,StrubF.,Altch´eF.,etal.Bootstrapyourownlatentanewapproachtoself-supervisedlearning[Z].Proceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems.Vancouver,BC,Canada;CurranAssociatesInc.2020:Article1786https://doi.org/10.48550/arXiv.2006.07733.[38]S.R.Li,Y.N.Zhao,J.Zhang,etal.,High-ordercorrelation-guidedslide-levelhistologyretrievalwithself-supervisedhashing,IEEETrans.PatternAnal.Mach.Intell.45(9)(2023)11008–11023,https://doi.org/10.1109/TPAMI.2023.3269810.[39]G.E.Hinton,S.Osindero,Y.-W.Teh,Afastlearningalgorithmfordeepbeliefnets,NeuralComput.18(7)(2006)1527–1554,https://doi.org/10.1162/neco.2006.18.7.1527.[40]W.C.Wang,E.Ahn,D.G.Feng,etal.,Areviewofpredictiveandcontrastiveself-supervisedlearningformedicalimages,Mach.Intell.Res.20(4)(2023)483–513,https://doi.org/10.1007/s11633-022-1406-4.[41]WuZ.,XiongY.,YuS.X.,etal.UnsupervisedFeatureLearningviaNon-parametricInstanceDiscrimination;proceedingsofthe2018IEEE/CVFConferenceonComputerVisionandPatternRecognition,F18-23June2018,2018[C].https://doi.org/10.1109/CVPR.2018.00393.[42]T.Chen,S.Kornblith,M.Norouzi,etal.,Asimpleframeworkforcontrastivelearningofvisualrepresentations,Int.Conf.Mach.Learn.119(2020),https://doi.org/10.48550/arXiv.2002.05709.[43]DehaeneO.,CamaraA.,MoindrotO.,etal.Self-SupervisionClosestheGapBetweenWeakandStrongSupervisioninHistology[arXiv][J].arXiv,2020:15pp.-pp.https://doi.org/10.48550/arXiv.2012.03583.[44]LiB.,LiY.,EliceiriK.W.Dual-streamMultipleInstanceLearningNetworkforWholeSlideImageClassificationwithSelf-supervisedContrastiveLearning;proceedingsofthe2021IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),F20-25June2021,2021[C].https://doi.org/10.48550/arXiv.2011.08939.[45]S.M.Alizadeh,M.S.Helfroush,H.Müller,AnovelSiamesedeephashingmodelforhistopathologyimageretrieval,ExpertSyst.Appl.225(2023),https://doi.org/10.1016/j.eswa.2023.120169.[46]CaronM.,MisraI.,MairalJ.,etal.Unsupervisedlearningofvisualfeaturesbycontrastingclusterassignments[arXiv][J].arXiv,2020:21pp.-pp.https://doi.org/10.48550/arXiv.2006.09882.[47]ChenX.,HeK.ExploringSimpleSiameseRepresentationLearning;proceedingsofthe2021IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),F20-25June2021,2021[C].https://doi.org/10.48550/arXiv.2011.10566.[48]K.K.Wickstrom,E.A.Ostmo,K.Radiya,etal.,Aclinicallymotivatedself-supervisedapproachforcontent-basedimageretrievalofCTliverimages,Comput.Med.ImagingGraph.107(2023),https://doi.org/10.1016/j.compmedimag.2023.102239.[49]HeK.,ZhangX.,RenS.,etal.Deepresiduallearningforimagerecognition;proceedingsofthe29thIEEEConferenceonComputerVisionandPatternRecognition,CVPR2016,June26,2016-July1,2016,LasVegas,NV,Unitedstates,F,2016[C].IEEEComputerSociety.https://doi.org/10.1109/CVPR.2016.90.[50]CakirF.,HeK.,XiaX.,etal.DeepMetricLearningtoRank;proceedingsofthe2019IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),F15-20June2019,2019[C].https://doi.org/10.1109/CVPR.2019.00196.[51]Y.Gong,S.Lazebnik,A.Gordo,etal.,Iterativequantization:aprocrusteanapproachtolearningbinarycodesforlarge-scaleimageretrieval,IEEETrans.PatternAnal.Mach.Intell.35(12)(2013)2916–2929,https://doi.org/10.1109/TPAMI.2012.193.[52]ShenY.,QinJ.,ChenJ.,etal.Auto-EncodingTwin-BottleneckHashing[Z].2020IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).2020:2815-24.10.1109/CVPR42600.2020.00289https://doi.org/10.1109/CVPR42600.2020.00289.[53]Y.K.Jang,N.I.Cho,Self-supervisedproductquantizationfordeepunsupervisedimageretrieval,2021IEEE/CVFInt.Conf.Comput.Vis.(ICCV)(2021)12065–12074,https://doi.org/10.48550/arXiv.2109.02244.XiYaWengiscurrentlypursuingthemaster’sdegreeinBiomedicalEngineeringofSichuanUniversity,Chengdu,China.Hercurrentresearchinterestsareinthefieldsofcom-putervision,imageretrieval,patternrecognitionandmachinelearning.X.Wengetal.

YanZhuangreceivedB.S.,M.S.andDr.degreeinBiomedicalEngineeringfromSichuanUniversity,China.ShehasbeenworkingasajointPh.D.candiateatthedepartmentofElec-tricalandComputerEngineeringattheUniversity.SheisanassistantresearcherintheDepartmentofBiomedicalEngi-neeringatSichuanUniversity.Hercurrentresearchinterestincludesdeeplearning,dataminingandartificialintelligenceinterpretability.RuiWangreceivedB.S.andM.S.degreesinBiomedicalEn-gineeringfromSichuanUniversity,China.SheiscurrentlyaresearchstaffmemberintheUltrasoundDepartmentattheGeneralHospitalofWesternTheaterCommand.Hercurrentresearchinterestsincludemedicaldataminingandtheappli-cationofartificialintelligenceinultrasoundmedicine.KeChenreceivedhisB.S.degreeinComputersciencein2004fromSouthwestJiaotongUniversity,Chengdu,China.HereceivedhisM.S.andPh.D.degreeinBiomedicalengineeringfromSichuanUniversity,Chengdu,China,in2007and2012,respectively.HeiscurrentlyanAssociateProfessorwiththeDepartmentofBiomedicalEngineering,SichuanUniversity.Hisresearchinterestsincludetheultrasonicimaging,ultra-sonictissuecharacterizationandbiomedicalimageprocessing.LinHanreceivedB.S.degreeinBiomedicalEngineeringfromSichuanUniversity,China,in2005,theMasterofEngineeringdegreeinBiomedicalEngineeringfromSichuanUniversity,China,in2018.Since2019,hehasbeenstudyingforaPh.DinBiomedicalEngineeringatSichuanUniversity.HisresearchinterestincludesUltrasoundimaging,Medicalimageprocess-ingandDeeplearning.ZhanHuareceivedabachelor’sdegreeinclinicalmedicinefromtheDepartmentofClinicalMedicineofShanxiMedicalCollegein1995,amaster’sdegreeinsurgeryfromtheChina-JapanFriendshipClinicalMedicalResearchInstitutein2001,andadoctorateinsurgeryfromtheUnionMedicalCollegeofChinain2004.HeisamemberoftheAmericanSocietyofColorectalSurgeons.HehasservedasaclinicalresearcheratMemorialSloanKetteringCancerCenterintheUnitedStates,andhasservedasaresidentphysician,attendingphysician,anddeputychiefphysicianinthegeneralsurgerydepartmentofChina-JapanFriendshipHospital.JiangliLinreceivedherM.SandPh.DdegreeinBiomedicalEngineeringfromSichuanUniversity,Chengdu,China.In2008,shehasbeenworkingasaPh.DresearcheratNIHandMayoMedicalCenterofSt.CloudStateUniversityintheUnitedStates(2008–2010).Sheiscurrentlyaprofessorandthedi-rectorofMedicalImagingandImageProcessingintheDepartmentofBiomedicalEngineering,SichuanUniversity.Herresearchinterestsincludethemedicalimagingprocessing,biosignalprocessing,bio-tissueviscoelasticitydetectionandUltrasound-enhancedelastography.X.Wengetal.