{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pseudo Label Generation (Google Colab)\n",
        "\n",
        "このノートブックは Google Colab 上で擬似ラベルを生成するためのテンプレートです。セルを 上から順番に実行してください。\n",
        "- 必要に応じて Google Drive から `.mat` をダウンロード / マウントして `data/` に配置してください。\n",
        "- 既存の `scripts/generate_pseudo_labels.py` と同じ処理をこの環境で再現します。\n",
        "- GPU ランタイムを有効にしておくと計算が高速になります (Runtime -> Change runtime type -> GPU)。\n",
        "- このリポジトリ一式を Colab 上で利用できるように (例: `!git clone ...` や `files.upload()` でアップロード) してから処理を実行してください。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89252e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet torch torchvision gdown h5py pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d76e7547",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure directories, shared Drive link, and download dataset if needed\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "SHARED_GDRIVE_URL = 'https://drive.google.com/file/d/1fa0gaEmbtGmqZ92L0EqzhH5LiMUAztix/view?usp=sharing'\n",
        "DATA_DIR = Path('data')\n",
        "ANNOTATIONS_DIR = Path('annotations')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "ANNOTATIONS_DIR.mkdir(exist_ok=True)\n",
        "DATASET_PATH = DATA_DIR / 'dataset.mat'\n",
        "ANNOTATIONS_CSV = ANNOTATIONS_DIR / 'dataset_labels.csv'\n",
        "\n",
        "# Default parameters used later in this notebook\n",
        "IMAGE_KEY = 'Acq/Amp'\n",
        "IMAGE_AXES = (0, 2, 1)\n",
        "PERCENTILE = 20.0\n",
        "SMOOTH_WINDOW = 5\n",
        "MIN_STABLE_LENGTH = 3\n",
        "LABEL_DTYPE = 'float32'\n",
        "\n",
        "if not DATASET_PATH.exists():\n",
        "    print(f'Downloading dataset to {DATASET_PATH} from Google Drive...')\n",
        "    cmd = ['gdown', '--fuzzy', SHARED_GDRIVE_URL, '-O', str(DATASET_PATH)]\n",
        "    result = subprocess.run(cmd, check=False)\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError('Failed to download dataset from Google Drive. Please verify the link.')\n",
        "else:\n",
        "    print('Dataset already present:', DATASET_PATH)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aee3240d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile generate_pseudo_labels.py\n",
        "# Write pseudo label generation script\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"Generate pseudo labels for ultrasound .mat sequences using full in-memory processing.\n",
        "\n",
        "Frames are loaded entirely into memory, motion scores are computed in a vectorised way,\n",
        "and results are exported as CSV with stability labels.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from typing import Optional, Sequence, Tuple\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _interpret_image_shape(\n",
        "    shape: Tuple[int, ...], override: Optional[Tuple[int, ...]] = None\n",
        ") -> Tuple[int, int, int, int, Tuple[int, ...]]:\n",
        "    if override is not None:\n",
        "        axes = tuple(override)\n",
        "        if len(axes) not in (3, 4):\n",
        "            raise ValueError(\"image_axes override must have length 3 or 4\")\n",
        "        if len(axes) != len(shape):\n",
        "            raise ValueError(\"image_axes override must match tensor rank\")\n",
        "        dims = [shape[a] for a in axes]\n",
        "        if len(axes) == 4:\n",
        "            n, c, h, w = dims\n",
        "            return n, c, h, w, axes\n",
        "        n, h, w = dims\n",
        "        return n, 1, h, w, axes\n",
        "\n",
        "    rank = len(shape)\n",
        "    if rank == 4:\n",
        "        candidates = [\n",
        "            (0, 3, 1, 2),\n",
        "            (0, 1, 2, 3),\n",
        "            (3, 2, 0, 1),\n",
        "            (3, 0, 1, 2),\n",
        "        ]\n",
        "        for axes in candidates:\n",
        "            n = shape[axes[0]]\n",
        "            c = shape[axes[1]]\n",
        "            h = shape[axes[2]]\n",
        "            w = shape[axes[3]]\n",
        "            if all(x > 0 for x in (n, c, h, w)):\n",
        "                return n, c, h, w, axes\n",
        "        raise ValueError(f\"Unable to infer N,C,H,W from shape {shape}\")\n",
        "    if rank == 3:\n",
        "        candidates = [\n",
        "            (0, 1, 2),\n",
        "            (0, 2, 1),\n",
        "            (2, 0, 1),\n",
        "            (2, 1, 0),\n",
        "            (1, 0, 2),\n",
        "            (1, 2, 0),\n",
        "        ]\n",
        "        for axes in candidates:\n",
        "            n = shape[axes[0]]\n",
        "            h = shape[axes[1]]\n",
        "            w = shape[axes[2]]\n",
        "            if all(x > 0 for x in (n, h, w)):\n",
        "                return n, 1, h, w, axes\n",
        "        raise ValueError(f\"Unable to infer N,H,W from shape {shape}\")\n",
        "    raise ValueError(f\"Unsupported image rank {rank}; expected 3D/4D, got {shape}\")\n",
        "\n",
        "\n",
        "def _reorder_full_stack(\n",
        "    dataset: h5py.Dataset,\n",
        "    axes: Tuple[int, ...],\n",
        "    dtype: str,\n",
        ") -> np.ndarray:\n",
        "    full = np.asarray(dataset, dtype=dtype)\n",
        "    target_axes = tuple(range(len(axes)))\n",
        "    reordered = np.moveaxis(full, axes, target_axes)\n",
        "    frames = reordered.reshape(reordered.shape[0], -1)\n",
        "    return frames\n",
        "\n",
        "\n",
        "def _moving_average(arr: np.ndarray, window: int) -> np.ndarray:\n",
        "    if window <= 1:\n",
        "        return arr.astype(np.float64, copy=False)\n",
        "    kernel = np.ones(window, dtype=np.float64) / float(window)\n",
        "    padded = np.pad(arr, (window // 2,), mode=\"edge\")\n",
        "    smoothed = np.convolve(padded, kernel, mode=\"valid\")\n",
        "    if smoothed.shape[0] > arr.shape[0]:\n",
        "        smoothed = smoothed[: arr.shape[0]]\n",
        "    return smoothed.astype(np.float64, copy=False)\n",
        "\n",
        "\n",
        "def compute_motion_scores(frames: np.ndarray) -> np.ndarray:\n",
        "    diff = np.abs(frames[1:] - frames[:-1])\n",
        "    scores = np.zeros(frames.shape[0], dtype=np.float64)\n",
        "    if diff.size:\n",
        "        scores[1:] = diff.mean(axis=1)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def enforce_min_segment(labels: np.ndarray, min_len: int) -> np.ndarray:\n",
        "    if min_len <= 1:\n",
        "        return labels\n",
        "    labels = labels.copy()\n",
        "    run_start = None\n",
        "    for idx, val in enumerate(labels):\n",
        "        if val == 1 and run_start is None:\n",
        "            run_start = idx\n",
        "        elif val == 0 and run_start is not None:\n",
        "            run_length = idx - run_start\n",
        "            if run_length < min_len:\n",
        "                labels[run_start:idx] = 0\n",
        "            run_start = None\n",
        "    if run_start is not None:\n",
        "        run_length = len(labels) - run_start\n",
        "        if run_length < min_len:\n",
        "            labels[run_start:] = 0\n",
        "    return labels\n",
        "\n",
        "\n",
        "def save_csv(path: Path, scores: np.ndarray, smooth_scores: np.ndarray, labels: np.ndarray) -> None:\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"frame\", \"label\", \"score\", \"smooth_score\"])\n",
        "        for idx, (lab, sc, sm) in enumerate(zip(labels, scores, smooth_scores)):\n",
        "            writer.writerow([idx, int(lab), float(sc), float(sm)])\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Pseudo-label ultrasound frames\")\n",
        "    parser.add_argument(\"--mat_path\", type=str, required=True, help=\"Path to v7.3 .mat file\")\n",
        "    parser.add_argument(\"--image_key\", type=str, default=\"Acq/Amp\", help=\"Dataset key inside the .mat file\")\n",
        "    parser.add_argument(\"--image_axes\", type=int, nargs='*', default=None, help=\"Axis override, e.g. 0 2 1\")\n",
        "    parser.add_argument(\"--output_csv\", type=str, required=True, help=\"Destination CSV for labels\")\n",
        "    parser.add_argument(\"--smooth_window\", type=int, default=5, help=\"Moving average window (1 to disable)\")\n",
        "    parser.add_argument(\"--percentile\", type=float, default=20.0, help=\"Percentile threshold for stability\")\n",
        "    parser.add_argument(\"--min_stable_length\", type=int, default=3, help=\"Minimum consecutive stable frames\")\n",
        "    parser.add_argument(\"--dtype\", type=str, default=\"float32\", help=\"Frame conversion dtype\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def generate_labels(args: argparse.Namespace) -> None:\n",
        "    image_axes = tuple(args.image_axes) if args.image_axes else None\n",
        "\n",
        "    with h5py.File(args.mat_path, \"r\") as f:\n",
        "        if args.image_key in f:\n",
        "            dataset = f[args.image_key]\n",
        "        else:\n",
        "            found = next((k for k in f.keys() if k.endswith(args.image_key)), None)\n",
        "            if found is None:\n",
        "                raise KeyError(f\"image_key '{args.image_key}' not found in {args.mat_path}\")\n",
        "            dataset = f[found]\n",
        "        _, _, _, _, axes = _interpret_image_shape(dataset.shape, override=image_axes)\n",
        "        frames = _reorder_full_stack(dataset, axes, args.dtype)\n",
        "\n",
        "    scores = compute_motion_scores(frames)\n",
        "    smooth_scores = _moving_average(scores, args.smooth_window)\n",
        "    threshold = np.percentile(smooth_scores, args.percentile)\n",
        "    labels = (smooth_scores <= threshold).astype(np.int32)\n",
        "    labels = enforce_min_segment(labels, args.min_stable_length)\n",
        "    save_csv(Path(args.output_csv), scores, smooth_scores, labels)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    generate_labels(args)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4459cc2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run pseudo label generation script and preview the first rows\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "RESULT_CSV = ANNOTATIONS_CSV\n",
        "cmd = [\n",
        "    'python',\n",
        "    'generate_pseudo_labels.py',\n",
        "    '--mat_path', str(DATASET_PATH),\n",
        "    '--image_key', IMAGE_KEY,\n",
        "    '--image_axes', *[str(a) for a in IMAGE_AXES],\n",
        "    '--output_csv', str(RESULT_CSV),\n",
        "    '--percentile', str(PERCENTILE),\n",
        "    '--smooth_window', str(SMOOTH_WINDOW),\n",
        "    '--min_stable_length', str(MIN_STABLE_LENGTH),\n",
        "    '--dtype', LABEL_DTYPE,\n",
        "]\n",
        "result = subprocess.run(cmd, check=False)\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError('Pseudo label generation failed. Please check earlier logs.')\n",
        "display(pd.read_csv(RESULT_CSV).head())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}