{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c06bc4",
   "metadata": {},
   "source": [
    "# SimSiam + FastAP Retrieval Pipeline\n",
    "\n",
    "End-to-end notebook for SimSiam pretraining, FastAP fine-tuning, embedding extraction, and retrieval evaluation. Designed for Colab execution with streamlined data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a836432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (execute in Colab as needed)\n",
    "!pip install --quiet gdown pandas matplotlib h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee729a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup, configuration, and reproducibility helpers\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as tvm\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "ANNOTATIONS_DIR = BASE_DIR / 'annotations'\n",
    "SAVE_DIR = BASE_DIR / 'save'\n",
    "RESULTS_DIR = SAVE_DIR / 'inference_results'\n",
    "SIMSIAM_DIR = SAVE_DIR / 'simsiam_stage'\n",
    "FASTAP_DIR = SAVE_DIR / 'fastap_stage'\n",
    "for path in (DATA_DIR, ANNOTATIONS_DIR, SAVE_DIR, RESULTS_DIR, SIMSIAM_DIR, FASTAP_DIR):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SHARED_DATASET_URL = 'https://drive.google.com/file/d/1fa0gaEmbtGmqZ92L0EqzhH5LiMUAztix/view?usp=sharing'\n",
    "SHARED_BREATH_HOLD_URL = ''  # 任意: invivo.jpg の共有リンク\n",
    "\n",
    "DATASET_PATH = DATA_DIR / 'dataset.mat'\n",
    "LABEL_CSV_PATH = ANNOTATIONS_DIR / 'dataset_labels.csv'\n",
    "BREATH_HOLD_IMAGE_PATH = DATA_DIR / 'invivo.jpg'\n",
    "CHECKPOINT_PATH = SAVE_DIR / 'latest.pth'\n",
    "\n",
    "PATHS = SimpleNamespace(\n",
    "    simsiam_dir=SIMSIAM_DIR,\n",
    "    fastap_dir=FASTAP_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    simsiam_latest=SIMSIAM_DIR / 'simsiam_latest.pth',\n",
    "    fastap_checkpoint=FASTAP_DIR / 'fastap_stage.pth',\n",
    "    embeddings=RESULTS_DIR / 'fastap_embeddings.npy',\n",
    "    metrics=RESULTS_DIR / 'fastap_metrics.json',\n",
    "    report=RESULTS_DIR / 'fastap_report.md',\n",
    "    simsiam_log=SIMSIAM_DIR / 'simsiam_train_log.json',\n",
    "    fastap_log=FASTAP_DIR / 'fastap_train_log.json',\n",
    ")\n",
    "\n",
    "CONFIG = SimpleNamespace(\n",
    "    backbone='resnet18',\n",
    "    image_key='Acq/Amp',\n",
    "    image_axes=(0, 2, 1),\n",
    "    normalize_255=False,\n",
    "    image_size=224,\n",
    "    proj_dim=2048,\n",
    "    emb_dim=256,\n",
    "    eval_batch_size=128,\n",
    "    dataloader=SimpleNamespace(\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True,\n",
    "    ),\n",
    "    simsiam=SimpleNamespace(\n",
    "        run=True,\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        lr=0.05,\n",
    "        momentum=0.9,\n",
    "        weight_decay=1e-4,\n",
    "        cosine_t_max=200,\n",
    "        amp=True,\n",
    "        resume=True,\n",
    "        save_every=10,\n",
    "    ),\n",
    "    fastap=SimpleNamespace(\n",
    "        run=True,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        optimizer='adamw',\n",
    "        scheduler='cosine',\n",
    "        freeze_backbone=True,\n",
    "        freeze_projector=False,\n",
    "        use_labels=False,\n",
    "        grad_clip=1.0,\n",
    "        amp=False,\n",
    "        num_bins=100,\n",
    "        sigma=0.05,\n",
    "        epsilon=1e-8,\n",
    "    ),\n",
    "    retrieval=SimpleNamespace(\n",
    "        topk=[1, 5, 10],\n",
    "        distance='cosine',\n",
    "    ),\n",
    "    seeds=SimpleNamespace(base=42),\n",
    ")\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.seeds.base)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "def ensure_data_availability() -> None:\n",
    "    if not DATASET_PATH.exists():\n",
    "        print(f'Downloading dataset to {DATASET_PATH} ...')\n",
    "        cmd = ['gdown', '--fuzzy', SHARED_DATASET_URL, '-O', str(DATASET_PATH)]\n",
    "        result = subprocess.run(cmd, check=False)\n",
    "        if result.returncode != 0:\n",
    "            raise RuntimeError('Failed to download dataset from Google Drive.')\n",
    "    else:\n",
    "        print('Dataset already present:', DATASET_PATH)\n",
    "\n",
    "    if not LABEL_CSV_PATH.exists():\n",
    "        alt_path = DATA_DIR / 'dataset_labels.csv'\n",
    "        if alt_path.exists():\n",
    "            alt_path.replace(LABEL_CSV_PATH)\n",
    "            print('Moved pseudo labels to annotations/:', LABEL_CSV_PATH)\n",
    "        else:\n",
    "            raise FileNotFoundError('annotations/dataset_labels.csv が見つかりません。事前に生成または配置してください。')\n",
    "    else:\n",
    "        print('Pseudo labels found:', LABEL_CSV_PATH)\n",
    "\n",
    "    if not BREATH_HOLD_IMAGE_PATH.exists():\n",
    "        moved = False\n",
    "        for candidate in (BASE_DIR / 'invivo.jpg',):\n",
    "            if candidate.exists():\n",
    "                candidate.replace(BREATH_HOLD_IMAGE_PATH)\n",
    "                print('Moved breath-hold image to data/:', BREATH_HOLD_IMAGE_PATH)\n",
    "                moved = True\n",
    "                break\n",
    "        if not moved and SHARED_BREATH_HOLD_URL:\n",
    "            print(f'Downloading breath-hold reference to {BREATH_HOLD_IMAGE_PATH} ...')\n",
    "            cmd = ['gdown', '--fuzzy', SHARED_BREATH_HOLD_URL, '-O', str(BREATH_HOLD_IMAGE_PATH)]\n",
    "            result = subprocess.run(cmd, check=False)\n",
    "            if result.returncode != 0:\n",
    "                raise RuntimeError('Failed to download breath-hold reference image.')\n",
    "    else:\n",
    "        print('Breath-hold reference image located at', BREATH_HOLD_IMAGE_PATH)\n",
    "\n",
    "ensure_data_availability()\n",
    "print('Artifacts will be stored under', SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d904a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset utilities and transforms (preloaded for speed)\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "def _resolve_image_dataset(f: h5py.File, key: str) -> h5py.Dataset:\n",
    "    if key in f:\n",
    "        return f[key]\n",
    "    for candidate in f.keys():\n",
    "        if candidate.endswith(key):\n",
    "            return f[candidate]\n",
    "    raise KeyError(f\"image_key '{key}' not found in {list(f.keys())}\")\n",
    "\n",
    "\n",
    "def _interpret_image_shape(\n",
    "    shape: Tuple[int, ...],\n",
    "    override: Optional[Tuple[int, ...]] = None,\n",
    ") -> Tuple[int, int, int, int, Tuple[int, ...]]:\n",
    "    if override is not None:\n",
    "        axes = tuple(override)\n",
    "        if len(axes) not in (3, 4):\n",
    "            raise ValueError('image_axes override must have length 3 or 4')\n",
    "        if len(axes) != len(shape):\n",
    "            raise ValueError('image_axes override must match dataset rank')\n",
    "        dims = [shape[a] for a in axes]\n",
    "        if len(axes) == 4:\n",
    "            n, c, h, w = dims\n",
    "            return n, c, h, w, axes\n",
    "        n, h, w = dims\n",
    "        return n, 1, h, w, axes\n",
    "    rank = len(shape)\n",
    "    if rank == 4:\n",
    "        candidates = [\n",
    "            (0, 3, 1, 2),\n",
    "            (0, 1, 2, 3),\n",
    "            (3, 2, 0, 1),\n",
    "            (3, 0, 1, 2),\n",
    "        ]\n",
    "        for axes in candidates:\n",
    "            n = shape[axes[0]]\n",
    "            c = shape[axes[1]]\n",
    "            h = shape[axes[2]]\n",
    "            w = shape[axes[3]]\n",
    "            if all(v > 0 for v in (n, c, h, w)):\n",
    "                return n, c, h, w, axes\n",
    "        raise ValueError(f'Unable to infer N,C,H,W from shape {shape}')\n",
    "    if rank == 3:\n",
    "        candidates = [\n",
    "            (0, 1, 2),\n",
    "            (0, 2, 1),\n",
    "            (2, 0, 1),\n",
    "            (2, 1, 0),\n",
    "            (1, 0, 2),\n",
    "            (1, 2, 0),\n",
    "        ]\n",
    "        for axes in candidates:\n",
    "            n = shape[axes[0]]\n",
    "            h = shape[axes[1]]\n",
    "            w = shape[axes[2]]\n",
    "            if all(v > 0 for v in (n, h, w)):\n",
    "                return n, 1, h, w, axes\n",
    "        raise ValueError(f'Unable to infer N,H,W from shape {shape}')\n",
    "    raise ValueError(f'Unsupported dataset rank {rank}; expected 3D or 4D')\n",
    "\n",
    "\n",
    "def _to_nchw(arr: np.ndarray, axes: Tuple[int, ...]) -> np.ndarray:\n",
    "    permuted = np.transpose(arr, axes)\n",
    "    if permuted.ndim == 3:\n",
    "        permuted = np.expand_dims(permuted, axis=1)\n",
    "    return permuted\n",
    "\n",
    "def load_frames_from_mat(\n",
    "    mat_files: Sequence[str],\n",
    "    image_key: str,\n",
    "    image_axes: Optional[Tuple[int, ...]] = None,\n",
    "    dtype: str = 'float32',\n",
    "    normalize_255: bool = False,\n",
    ") -> np.ndarray:\n",
    "    buffers: List[np.ndarray] = []\n",
    "    for path in mat_files:\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            ds = _resolve_image_dataset(f, image_key)\n",
    "            n, c, h, w, axes = _interpret_image_shape(ds.shape, override=image_axes)\n",
    "            arr = np.asarray(ds)\n",
    "            arr = _to_nchw(arr, axes)\n",
    "            if arr.dtype.kind in ('u', 'i'):\n",
    "                arr = arr.astype(dtype, copy=False)\n",
    "                if normalize_255:\n",
    "                    arr = arr / 255.0\n",
    "            else:\n",
    "                arr = arr.astype(dtype, copy=False)\n",
    "            buffers.append(arr)\n",
    "    if not buffers:\n",
    "        raise ValueError('No frames were loaded from the provided .mat files.')\n",
    "    stacked = np.concatenate(buffers, axis=0)\n",
    "    return stacked\n",
    "\n",
    "\n",
    "class PreloadedMatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mat_files: Sequence[str],\n",
    "        image_key: str,\n",
    "        image_axes: Optional[Tuple[int, ...]] = None,\n",
    "        normalize_255: bool = False,\n",
    "        transform=None,\n",
    "        dtype: str = 'float32',\n",
    "    ):\n",
    "        self.data = load_frames_from_mat(\n",
    "            mat_files, image_key, image_axes=image_axes, dtype=dtype, normalize_255=normalize_255\n",
    "        )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get_raw(self, idx: int) -> np.ndarray:\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        chw = self.get_raw(idx)\n",
    "        if self.transform is not None:\n",
    "            return self.transform(chw)\n",
    "        return torch.from_numpy(chw)\n",
    "\n",
    "    def view(self, transform):\n",
    "        return PreloadedDatasetView(self, transform)\n",
    "\n",
    "\n",
    "class PreloadedDatasetView(torch.utils.data.Dataset):\n",
    "    def __init__(self, base: PreloadedMatDataset, transform):\n",
    "        self.base = base\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        chw = self.base.get_raw(idx)\n",
    "        if self.transform is not None:\n",
    "            return self.transform(chw)\n",
    "        return torch.from_numpy(chw)\n",
    "\n",
    "\n",
    "def bmode_normalize(chw: np.ndarray) -> np.ndarray:\n",
    "    x = np.abs(chw.astype(np.float32))\n",
    "    x = x / (x.max() + 1e-12)\n",
    "    x = 20.0 * np.log10(x + 1e-12)\n",
    "    x = np.clip((x + 60.0) / 60.0, 0.0, 1.0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def to_pil_3ch_from_chw01(chw01: np.ndarray) -> Image.Image:\n",
    "    if chw01.shape[0] == 1:\n",
    "        chw01 = np.repeat(chw01, 3, axis=0)\n",
    "    hwc = np.transpose(chw01, (1, 2, 0))\n",
    "    hwc255 = (hwc * 255.0).astype(np.uint8)\n",
    "    return Image.fromarray(hwc255)\n",
    "\n",
    "\n",
    "def build_simsiam_augmentation(image_size: int = 224) -> T.Compose:\n",
    "    color_jitter = T.ColorJitter(0.4, 0.4, 0.4, 0.1)\n",
    "    blur_kernel = max(3, int(image_size * 0.1))\n",
    "    if blur_kernel % 2 == 0:\n",
    "        blur_kernel += 1\n",
    "    return T.Compose([\n",
    "        T.RandomResizedCrop(image_size, scale=(0.5, 1.0), interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomApply([color_jitter], p=0.8),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        T.RandomApply([T.GaussianBlur(kernel_size=blur_kernel, sigma=(0.1, 2.0))], p=0.5),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "\n",
    "class TwoViewTransform:\n",
    "    def __init__(self, transform_q: T.Compose, transform_k: Optional[T.Compose] = None):\n",
    "        self.transform_q = transform_q\n",
    "        self.transform_k = transform_k or transform_q\n",
    "\n",
    "    def __call__(self, chw: np.ndarray):\n",
    "        chw01 = bmode_normalize(chw)\n",
    "        pil = to_pil_3ch_from_chw01(chw01)\n",
    "        view_q = self.transform_q(pil)\n",
    "        view_k = self.transform_k(pil)\n",
    "        return view_q, view_k\n",
    "\n",
    "\n",
    "class TwoViewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset: PreloadedMatDataset, pair_transform: TwoViewTransform):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.pair_transform = pair_transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        chw = self.base_dataset.get_raw(idx)\n",
    "        return self.pair_transform(chw)\n",
    "\n",
    "\n",
    "class FastAPPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset: PreloadedMatDataset, pair_transform: TwoViewTransform, labels: Optional[np.ndarray] = None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.pair_transform = pair_transform\n",
    "        if labels is not None and len(labels) != len(base_dataset):\n",
    "            raise ValueError('Labels length must match dataset length.')\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        chw = self.base_dataset.get_raw(idx)\n",
    "        view_q, view_k = self.pair_transform(chw)\n",
    "        label = -1 if self.labels is None else int(self.labels[idx])\n",
    "        return view_q, view_k, idx, label\n",
    "\n",
    "\n",
    "def build_eval_transform(image_size: int = 224) -> T.Compose:\n",
    "    return T.Compose([\n",
    "        T.Lambda(lambda x: to_pil_3ch_from_chw01(bmode_normalize(x))),\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(image_size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "eval_transform = build_eval_transform(CONFIG.image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde814e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model components, losses, and training utilities\n",
    "from typing import Any, Dict, Optional, Sequence\n",
    "\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim: int, hid_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim, bias=False),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, out_dim, bias=False),\n",
    "            nn.BatchNorm1d(out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim: int, hid_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim, bias=False),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EmbeddingHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, normalize: bool = True) -> torch.Tensor:\n",
    "        z = self.net(x)\n",
    "        if normalize:\n",
    "            z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "\n",
    "class FastAPModel(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, projector: Projector, embedding_head: EmbeddingHead, predictor: Predictor):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projector = projector\n",
    "        self.predictor = predictor\n",
    "        self.embedding_head = embedding_head\n",
    "\n",
    "    def forward_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.backbone(x)\n",
    "        return torch.flatten(feats, 1)\n",
    "\n",
    "    def forward_projector(self, x: torch.Tensor, normalize: bool = False) -> torch.Tensor:\n",
    "        h = self.forward_backbone(x)\n",
    "        z = self.projector(h)\n",
    "        if normalize:\n",
    "            z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "    def forward_embedding(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.forward_projector(x, normalize=False)\n",
    "        emb = self.embedding_head(z, normalize=True)\n",
    "        return emb\n",
    "\n",
    "    def encode(self, x: torch.Tensor, stage: str = 'embedding') -> torch.Tensor:\n",
    "        if stage == 'backbone':\n",
    "            return self.forward_backbone(x)\n",
    "        if stage == 'projector':\n",
    "            return self.forward_projector(x, normalize=True)\n",
    "        if stage == 'embedding':\n",
    "            return self.forward_embedding(x)\n",
    "        raise ValueError(f'Unknown encode stage: {stage}')\n",
    "\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(self, backbone_name: str = 'resnet18', proj_dim: int = 2048):\n",
    "        super().__init__()\n",
    "        if backbone_name == 'resnet18':\n",
    "            backbone = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            feat_dim = 512\n",
    "        elif backbone_name == 'resnet50':\n",
    "            backbone = tvm.resnet50(weights=tvm.ResNet50_Weights.IMAGENET1K_V2)\n",
    "            feat_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported backbone: {backbone_name}')\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.feat_dim = feat_dim\n",
    "        self.projector = Projector(in_dim=feat_dim, hid_dim=proj_dim, out_dim=proj_dim)\n",
    "        self.predictor = Predictor(in_dim=proj_dim, hid_dim=512, out_dim=proj_dim)\n",
    "\n",
    "    def forward_backbone(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.backbone(x)\n",
    "        h = torch.flatten(h, 1)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor):\n",
    "        h1 = self.forward_backbone(x1)\n",
    "        z1 = self.projector(h1)\n",
    "        p1 = self.predictor(z1)\n",
    "        h2 = self.forward_backbone(x2)\n",
    "        z2 = self.projector(h2)\n",
    "        p2 = self.predictor(z2)\n",
    "        return p1, z1, p2, z2\n",
    "\n",
    "\n",
    "def build_fastap_model(backbone_name: str, proj_dim: int, emb_dim: int) -> FastAPModel:\n",
    "    simsiam = SimSiam(backbone_name=backbone_name, proj_dim=proj_dim)\n",
    "    embedding_head = EmbeddingHead(in_dim=proj_dim, hidden_dim=proj_dim, out_dim=emb_dim)\n",
    "    return FastAPModel(simsiam.backbone, simsiam.projector, embedding_head, simsiam.predictor)\n",
    "\n",
    "\n",
    "def build_simsiam(backbone_name: str, proj_dim: int) -> SimSiam:\n",
    "    return SimSiam(backbone_name=backbone_name, proj_dim=proj_dim)\n",
    "\n",
    "\n",
    "def negative_cosine(p: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "    z = z.detach()\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def make_positive_mask(ids_or_labels: torch.Tensor) -> torch.BoolTensor:\n",
    "    ids = ids_or_labels.view(-1, 1)\n",
    "    mask = ids == ids.T\n",
    "    diag = torch.eye(ids.size(0), dtype=torch.bool, device=ids.device)\n",
    "    mask = mask & ~diag\n",
    "    return mask\n",
    "\n",
    "\n",
    "def fastap_loss(\n",
    "    embeddings: torch.Tensor,\n",
    "    pos_mask: torch.BoolTensor,\n",
    "    *,\n",
    "    num_bins: int = 50,\n",
    "    sigma: float = 0.05,\n",
    "    epsilon: float = 1e-8,\n",
    ") -> torch.Tensor:\n",
    "    if embeddings.dim() != 2:\n",
    "        raise ValueError('Expected embeddings of shape [N, D].')\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "    device = embeddings.device\n",
    "    n = embeddings.size(0)\n",
    "\n",
    "    sim = embeddings @ embeddings.T\n",
    "    diag = torch.eye(n, device=device, dtype=torch.bool)\n",
    "    sim = sim.masked_fill(diag, -1.0)\n",
    "\n",
    "    pos_mask = pos_mask.to(device) & ~diag\n",
    "\n",
    "    bin_centers = torch.linspace(-1.0, 1.0, steps=num_bins, device=device)\n",
    "    bin_width = 2.0 / num_bins\n",
    "    half_width = bin_width / 2.0\n",
    "\n",
    "    s = sim.unsqueeze(-1)\n",
    "    c = bin_centers.view(1, 1, -1)\n",
    "    assign = torch.sigmoid((s - (c - half_width)) / sigma) - torch.sigmoid((s - (c + half_width)) / sigma)\n",
    "    assign = assign.clamp(min=0.0)\n",
    "\n",
    "    valid_mask = (~diag).unsqueeze(-1)\n",
    "    assign = assign * valid_mask\n",
    "\n",
    "    pos_weights = assign * pos_mask.unsqueeze(-1).float()\n",
    "    all_weights = assign\n",
    "\n",
    "    H = all_weights.sum(dim=1)\n",
    "    P = pos_weights.sum(dim=1)\n",
    "\n",
    "    cum_H = torch.cumsum(H, dim=1)\n",
    "    cum_P = torch.cumsum(P, dim=1)\n",
    "\n",
    "    pos_counts = pos_mask.float().sum(dim=1)\n",
    "    valid_queries = pos_counts > 0\n",
    "\n",
    "    precision = (cum_P + epsilon) / (cum_H + epsilon)\n",
    "    recall = (cum_P + epsilon) / (pos_counts.unsqueeze(-1) + epsilon)\n",
    "    delta_recall = recall[:, :1]\n",
    "    if recall.size(1) > 1:\n",
    "        delta_recall = torch.cat([delta_recall, recall[:, 1:] - recall[:, :-1]], dim=1)\n",
    "\n",
    "    ap = (precision * delta_recall).sum(dim=1)\n",
    "    ap = ap[valid_queries]\n",
    "\n",
    "    if ap.numel() == 0:\n",
    "        return embeddings.new_tensor(0.0, requires_grad=True)\n",
    "    return -ap.mean()\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainLog:\n",
    "    epochs: List[int]\n",
    "    losses: List[float]\n",
    "    lrs: List[float]\n",
    "    elapsed: List[float]\n",
    "    config: Dict[str, Any]\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            'epochs': self.epochs,\n",
    "            'losses': self.losses,\n",
    "            'lrs': self.lrs,\n",
    "            'elapsed': self.elapsed,\n",
    "            'config': self.config,\n",
    "        }\n",
    "\n",
    "\n",
    "def train_simsiam_stage(\n",
    "    dataloader: DataLoader,\n",
    "    model: SimSiam,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    epochs: int,\n",
    "    device: torch.device,\n",
    "    *,\n",
    "    amp_enabled: bool = True,\n",
    "    log_every: int = 50,\n",
    ") -> TrainLog:\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp_enabled and device.type == 'cuda'))\n",
    "    history = TrainLog(epochs=[], losses=[], lrs=[], elapsed=[], config={'stage': 'simsiam'})\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start = time.time()\n",
    "        for step, (view_q, view_k) in enumerate(dataloader, start=1):\n",
    "            view_q = view_q.to(device, non_blocking=True)\n",
    "            view_k = view_k.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(amp_enabled and device.type == 'cuda')):\n",
    "                p1, z1, p2, z2 = model(view_q, view_k)\n",
    "                loss = 0.5 * negative_cosine(p1, z2) + 0.5 * negative_cosine(p2, z1)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += float(loss.item())\n",
    "            if log_every and step % log_every == 0:\n",
    "                avg = running_loss / step\n",
    "                print(f'Epoch {epoch:02d} step {step:04d}/{len(dataloader):04d} | loss={avg:.4f}')\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        elapsed = time.time() - start\n",
    "        avg_loss = running_loss / max(1, len(dataloader))\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        history.epochs.append(epoch)\n",
    "        history.losses.append(avg_loss)\n",
    "        history.lrs.append(lr)\n",
    "        history.elapsed.append(elapsed)\n",
    "        print(f'Epoch {epoch:02d}/{epochs:02d} completed | loss={avg_loss:.4f} | lr={lr:.3e} | time={elapsed:.1f}s')\n",
    "    return history\n",
    "\n",
    "\n",
    "def train_fastap_stage(\n",
    "    dataloader: DataLoader,\n",
    "    model: FastAPModel,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    epochs: int,\n",
    "    cfg,\n",
    "    *,\n",
    "    log_every: int = 50,\n",
    ") -> TrainLog:\n",
    "    device = next(model.parameters()).device\n",
    "    amp_enabled = getattr(cfg, 'amp', False) and device.type == 'cuda'\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
    "    history = TrainLog(epochs=[], losses=[], lrs=[], elapsed=[], config={\n",
    "        'stage': 'fastap',\n",
    "        'num_bins': cfg.num_bins,\n",
    "        'sigma': cfg.sigma,\n",
    "        'epsilon': getattr(cfg, 'epsilon', 1e-8),\n",
    "        'use_labels': cfg.use_labels,\n",
    "    })\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start = time.time()\n",
    "        for step, (view_q, view_k, idx, labels) in enumerate(dataloader, start=1):\n",
    "            view_q = view_q.to(device, non_blocking=True)\n",
    "            view_k = view_k.to(device, non_blocking=True)\n",
    "            ids = idx.to(device, non_blocking=True)\n",
    "            labels_tensor = labels.to(device, non_blocking=True)\n",
    "            use_labels = cfg.use_labels and (labels_tensor >= 0).any()\n",
    "            if use_labels:\n",
    "                labels_tensor = torch.where(labels_tensor >= 0, labels_tensor, ids)\n",
    "                pair_ids = torch.cat([labels_tensor, labels_tensor], dim=0)\n",
    "            else:\n",
    "                pair_ids = torch.cat([ids, ids], dim=0)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
    "                emb_q = model.forward_embedding(view_q)\n",
    "                emb_k = model.forward_embedding(view_k)\n",
    "                embeddings = torch.cat([emb_q, emb_k], dim=0)\n",
    "                pos_mask = make_positive_mask(pair_ids)\n",
    "                loss = fastap_loss(\n",
    "                    embeddings,\n",
    "                    pos_mask,\n",
    "                    num_bins=cfg.num_bins,\n",
    "                    sigma=cfg.sigma,\n",
    "                    epsilon=getattr(cfg, 'epsilon', 1e-8),\n",
    "                )\n",
    "            if amp_enabled:\n",
    "                scaler.scale(loss).backward()\n",
    "                if cfg.grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if cfg.grad_clip is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
    "                optimizer.step()\n",
    "            if scheduler is not None and getattr(cfg, 'scheduler_step', 'epoch') == 'step':\n",
    "                scheduler.step()\n",
    "            running_loss += float(loss.item())\n",
    "            if log_every and step % log_every == 0:\n",
    "                avg = running_loss / step\n",
    "                print(f'Epoch {epoch:02d} step {step:04d}/{len(dataloader):04d} | loss={avg:.4f}')\n",
    "        if scheduler is not None and getattr(cfg, 'scheduler_step', 'epoch') == 'epoch':\n",
    "            scheduler.step()\n",
    "        elapsed = time.time() - start\n",
    "        avg_loss = running_loss / max(1, len(dataloader))\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        history.epochs.append(epoch)\n",
    "        history.losses.append(avg_loss)\n",
    "        history.lrs.append(lr)\n",
    "        history.elapsed.append(elapsed)\n",
    "        print(f'Epoch {epoch:02d}/{epochs:02d} completed | loss={avg_loss:.4f} | lr={lr:.3e} | time={elapsed:.1f}s')\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_map(embeddings: np.ndarray, labels: np.ndarray, topk: Optional[Sequence[int]] = None) -> Dict[str, Any]:\n",
    "    if embeddings.ndim != 2:\n",
    "        raise ValueError('Expected embeddings with shape [N, D].')\n",
    "    if labels.ndim != 1 or labels.shape[0] != embeddings.shape[0]:\n",
    "        raise ValueError('Labels must have shape [N].')\n",
    "    embeddings = embeddings.astype(np.float32)\n",
    "    labels = labels.astype(np.int64)\n",
    "    sim = embeddings @ embeddings.T\n",
    "    np.fill_diagonal(sim, -np.inf)\n",
    "    order = np.argsort(-sim, axis=1)\n",
    "    ap_list = []\n",
    "    precision_at_k: Dict[int, float] = {}\n",
    "    recall_at_k: Dict[int, float] = {}\n",
    "    if topk is None:\n",
    "        topk = [1, 5, 10]\n",
    "    topk = sorted(set(int(k) for k in topk if k > 0))\n",
    "    for i in range(sim.shape[0]):\n",
    "        rel = (labels == labels[i]).astype(np.int32)\n",
    "        rel[i] = 0\n",
    "        ranked = order[i]\n",
    "        rel_ranked = rel[ranked]\n",
    "        n_rel = rel_ranked.sum()\n",
    "        if n_rel == 0:\n",
    "            ap_list.append(0.0)\n",
    "        else:\n",
    "            cumsum = np.cumsum(rel_ranked)\n",
    "            precision = cumsum / (np.arange(len(rel_ranked)) + 1)\n",
    "            ap = (precision * rel_ranked).sum() / n_rel\n",
    "            ap_list.append(float(ap))\n",
    "        for k in topk:\n",
    "            hits = rel_ranked[:k]\n",
    "            h_sum = hits.sum()\n",
    "            precision_at_k.setdefault(k, 0.0)\n",
    "            recall_at_k.setdefault(k, 0.0)\n",
    "            precision_at_k[k] += float(h_sum) / k\n",
    "            if n_rel > 0:\n",
    "                recall_at_k[k] += float(h_sum) / n_rel\n",
    "    num_queries = sim.shape[0]\n",
    "    metrics = {\n",
    "        'mAP': float(np.mean(ap_list)),\n",
    "        'per_query_ap': ap_list,\n",
    "        'precision_at_k': {k: precision_at_k[k] / num_queries for k in topk},\n",
    "        'recall_at_k': {k: recall_at_k[k] / num_queries for k in topk},\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def extract_embeddings(model: FastAPModel, loader: DataLoader, device: torch.device, stage: str = 'embedding') -> np.ndarray:\n",
    "    model.eval()\n",
    "    outputs: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            feats = model.encode(xb, stage=stage)\n",
    "            outputs.append(feats.cpu().numpy())\n",
    "    return np.concatenate(outputs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3fa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets, run SimSiam pretraining and FastAP fine-tuning\n",
    "mat_files = [str(DATASET_PATH)]\n",
    "raw_dataset = PreloadedMatDataset(\n",
    "    mat_files,\n",
    "    image_key=CONFIG.image_key,\n",
    "    image_axes=CONFIG.image_axes,\n",
    "    normalize_255=CONFIG.normalize_255,\n",
    "    transform=None,\n",
    ")\n",
    "print('Loaded frames:', len(raw_dataset))\n",
    "\n",
    "labels_df = pd.read_csv(LABEL_CSV_PATH).set_index('frame').sort_index()\n",
    "frame_labels = labels_df.loc[range(len(raw_dataset)), 'label'].to_numpy().astype(np.int64)\n",
    "\n",
    "simsiam_log = None\n",
    "fastap_log = None\n",
    "\n",
    "# Stage 1: SimSiam pretraining\n",
    "if CONFIG.simsiam.run:\n",
    "    pair_transform = TwoViewTransform(build_simsiam_augmentation(CONFIG.image_size))\n",
    "    simsiam_dataset = TwoViewDataset(raw_dataset, pair_transform)\n",
    "    simsiam_loader = DataLoader(\n",
    "        simsiam_dataset,\n",
    "        batch_size=CONFIG.simsiam.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=CONFIG.dataloader.drop_last,\n",
    "        num_workers=CONFIG.dataloader.num_workers,\n",
    "        pin_memory=CONFIG.dataloader.pin_memory,\n",
    "    )\n",
    "    if len(simsiam_loader) == 0:\n",
    "        raise RuntimeError('SimSiam DataLoader returned 0 batches. Reduce batch size or disable drop_last.')\n",
    "    simsiam_model = build_simsiam(CONFIG.backbone, CONFIG.proj_dim).to(DEVICE)\n",
    "    resume_path = PATHS.simsiam_latest\n",
    "    if CONFIG.simsiam.resume and resume_path.exists():\n",
    "        ckpt = torch.load(resume_path, map_location=DEVICE)\n",
    "        state_dict = ckpt.get('state_dict', ckpt)\n",
    "        load_result = simsiam_model.load_state_dict(state_dict, strict=False)\n",
    "        print('Resumed SimSiam checkpoint:', resume_path)\n",
    "        if getattr(load_result, 'missing_keys', None):\n",
    "            print('  Missing keys:', load_result.missing_keys)\n",
    "        if getattr(load_result, 'unexpected_keys', None):\n",
    "            print('  Unexpected keys:', load_result.unexpected_keys)\n",
    "    optimizer = torch.optim.SGD(\n",
    "        simsiam_model.parameters(),\n",
    "        lr=CONFIG.simsiam.lr,\n",
    "        momentum=CONFIG.simsiam.momentum,\n",
    "        weight_decay=CONFIG.simsiam.weight_decay,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG.simsiam.cosine_t_max)\n",
    "    log_every = max(1, len(simsiam_loader) // 5)\n",
    "    simsiam_log = train_simsiam_stage(\n",
    "        dataloader=simsiam_loader,\n",
    "        model=simsiam_model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epochs=CONFIG.simsiam.epochs,\n",
    "        device=DEVICE,\n",
    "        amp_enabled=CONFIG.simsiam.amp,\n",
    "        log_every=log_every,\n",
    "    )\n",
    "    torch.save(simsiam_log.to_dict(), PATHS.simsiam_log)\n",
    "    torch.save({'epoch': simsiam_log.epochs[-1] if simsiam_log.epochs else 0, 'state_dict': simsiam_model.state_dict()}, PATHS.simsiam_latest)\n",
    "    torch.save({'state_dict': simsiam_model.state_dict()}, CHECKPOINT_PATH)\n",
    "    print('SimSiam checkpoint saved to', PATHS.simsiam_latest)\n",
    "else:\n",
    "    print('Skipping SimSiam pretraining stage.')\n",
    "    simsiam_model = None\n",
    "\n",
    "# Load SimSiam weights for FastAP initialisation\n",
    "simsim_state_dict = None\n",
    "if CONFIG.simsiam.run and simsiam_model is not None:\n",
    "    simsim_state_dict = simsiam_model.state_dict()\n",
    "else:\n",
    "    fallback_paths = [PATHS.simsiam_latest, CHECKPOINT_PATH]\n",
    "    for candidate in fallback_paths:\n",
    "        if candidate.exists():\n",
    "            ckpt = torch.load(candidate, map_location=DEVICE)\n",
    "            simsim_state_dict = ckpt.get('state_dict', ckpt)\n",
    "            print('Using SimSiam state from', candidate)\n",
    "            break\n",
    "\n",
    "# Stage 2: FastAP fine-tuning\n",
    "model = build_fastap_model(CONFIG.backbone, CONFIG.proj_dim, CONFIG.emb_dim).to(DEVICE)\n",
    "if simsim_state_dict is not None:\n",
    "    load_result = model.load_state_dict(simsim_state_dict, strict=False)\n",
    "    print('Loaded SimSiam weights into FastAP model.')\n",
    "    if getattr(load_result, 'missing_keys', None):\n",
    "        print('  Missing keys:', load_result.missing_keys)\n",
    "    if getattr(load_result, 'unexpected_keys', None):\n",
    "        print('  Unexpected keys:', load_result.unexpected_keys)\n",
    "else:\n",
    "    print('Warning: no SimSiam weights available; FastAP model initialised randomly.')\n",
    "\n",
    "if CONFIG.fastap.freeze_backbone:\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "if CONFIG.fastap.freeze_projector:\n",
    "    for param in model.projector.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print('Trainable parameter count:', sum(p.numel() for p in trainable_params))\n",
    "\n",
    "if CONFIG.fastap.run:\n",
    "    pair_transform = TwoViewTransform(build_simsiam_augmentation(CONFIG.image_size))\n",
    "    labels_for_training = frame_labels if CONFIG.fastap.use_labels else None\n",
    "    fastap_dataset = FastAPPairDataset(raw_dataset, pair_transform, labels=labels_for_training)\n",
    "    train_loader = DataLoader(\n",
    "        fastap_dataset,\n",
    "        batch_size=CONFIG.fastap.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=CONFIG.dataloader.drop_last,\n",
    "        num_workers=CONFIG.dataloader.num_workers,\n",
    "        pin_memory=CONFIG.dataloader.pin_memory,\n",
    "    )\n",
    "    if len(train_loader) == 0:\n",
    "        raise RuntimeError('FastAP DataLoader returned 0 batches. Reduce batch size or disable drop_last.')\n",
    "    if not trainable_params:\n",
    "        raise RuntimeError('No parameters left to optimise after freezing selections.')\n",
    "    if CONFIG.fastap.optimizer.lower() == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=CONFIG.fastap.lr, weight_decay=CONFIG.fastap.weight_decay)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(trainable_params, lr=CONFIG.fastap.lr, momentum=0.9, weight_decay=CONFIG.fastap.weight_decay)\n",
    "    scheduler = None\n",
    "    scheduler_step = 'epoch'\n",
    "    if CONFIG.fastap.scheduler == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG.fastap.epochs)\n",
    "        scheduler_step = 'epoch'\n",
    "    fastap_cfg = SimpleNamespace(\n",
    "        num_bins=CONFIG.fastap.num_bins,\n",
    "        sigma=CONFIG.fastap.sigma,\n",
    "        epsilon=CONFIG.fastap.epsilon,\n",
    "        use_labels=CONFIG.fastap.use_labels,\n",
    "        grad_clip=CONFIG.fastap.grad_clip,\n",
    "        amp=CONFIG.fastap.amp,\n",
    "        scheduler_step=scheduler_step,\n",
    "    )\n",
    "    log_every = max(1, len(train_loader) // 5)\n",
    "    fastap_log = train_fastap_stage(\n",
    "        dataloader=train_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epochs=CONFIG.fastap.epochs,\n",
    "        cfg=fastap_cfg,\n",
    "        log_every=log_every,\n",
    "    )\n",
    "    torch.save(fastap_log.to_dict(), PATHS.fastap_log)\n",
    "    torch.save(\n",
    "        {\n",
    "            'model_state': model.state_dict(),\n",
    "            'config': {\n",
    "                'proj_dim': CONFIG.proj_dim,\n",
    "                'emb_dim': CONFIG.emb_dim,\n",
    "                'fastap': vars(CONFIG.fastap),\n",
    "                'backbone': CONFIG.backbone,\n",
    "            },\n",
    "        },\n",
    "        PATHS.fastap_checkpoint,\n",
    "    )\n",
    "    torch.save({'state_dict': model.state_dict()}, CHECKPOINT_PATH)\n",
    "    print('FastAP checkpoint saved to', PATHS.fastap_checkpoint)\n",
    "else:\n",
    "    print('Skipping FastAP fine-tuning stage.')\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591abdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings and evaluate retrieval metrics\n",
    "eval_dataset = raw_dataset.view(eval_transform)\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=CONFIG.eval_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG.dataloader.num_workers,\n",
    "    pin_memory=CONFIG.dataloader.pin_memory,\n",
    ")\n",
    "\n",
    "embeddings = extract_embeddings(model, eval_loader, DEVICE, stage='embedding')\n",
    "np.save(PATHS.embeddings, embeddings)\n",
    "print('Embeddings shape:', embeddings.shape)\n",
    "print('Saved embeddings to', PATHS.embeddings)\n",
    "\n",
    "labels_for_eval = frame_labels[: len(eval_dataset)].astype(np.int64)\n",
    "metrics = evaluate_map(embeddings, labels_for_eval, topk=CONFIG.retrieval.topk)\n",
    "with open(PATHS.metrics, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('mAP:', metrics['mAP'])\n",
    "print('Metrics saved to', PATHS.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar-frame retrieval helpers and example queries\n",
    "from typing import Tuple\n",
    "\n",
    "TOPK = max(CONFIG.retrieval.topk)\n",
    "\n",
    "retrieval_transform = T.Compose([\n",
    "    T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(CONFIG.image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_external_image(path: str) -> torch.Tensor:\n",
    "    pil = Image.open(path).convert('RGB')\n",
    "    return retrieval_transform(pil)\n",
    "\n",
    "def embed_tensor(model: FastAPModel, img: torch.Tensor) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.forward_embedding(img.unsqueeze(0).to(DEVICE))\n",
    "    return out.cpu().numpy()[0]\n",
    "\n",
    "def search_similar(emb_matrix: np.ndarray, query_emb: np.ndarray, topk: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    emb_norm = emb_matrix / (np.linalg.norm(emb_matrix, axis=1, keepdims=True) + 1e-12)\n",
    "    query_norm = query_emb / (np.linalg.norm(query_emb) + 1e-12)\n",
    "    scores = emb_norm @ query_norm\n",
    "    top_indices = np.argsort(-scores)[:topk]\n",
    "    return top_indices, scores[top_indices]\n",
    "\n",
    "retrieval_summary = {\n",
    "    'query_index': 0,\n",
    "    'internal': [],\n",
    "    'external_query': None,\n",
    "}\n",
    "\n",
    "query_index = retrieval_summary['query_index']\n",
    "indices, scores = search_similar(embeddings, embeddings[query_index], topk=min(TOPK, len(embeddings)))\n",
    "print(f'Query frame index: {query_index}')\n",
    "for rank, (idx, score) in enumerate(zip(indices, scores), start=1):\n",
    "    label = int(frame_labels[idx]) if idx < len(frame_labels) else -1\n",
    "    print(f'Rank {rank:02d}: frame={idx} | score={score:.4f} | label={label}')\n",
    "    retrieval_summary['internal'].append({\n",
    "        'rank': int(rank),\n",
    "        'frame': int(idx),\n",
    "        'score': float(score),\n",
    "        'label': int(label),\n",
    "    })\n",
    "\n",
    "if BREATH_HOLD_IMAGE_PATH.exists():\n",
    "    external_tensor = preprocess_external_image(str(BREATH_HOLD_IMAGE_PATH))\n",
    "    external_embedding = embed_tensor(model, external_tensor)\n",
    "    ext_indices, ext_scores = search_similar(embeddings, external_embedding, topk=min(TOPK, len(embeddings)))\n",
    "    ext_results = []\n",
    "    print('\\nExternal query (invivo.jpg) top matches:')\n",
    "    for rank, (idx, score) in enumerate(zip(ext_indices, ext_scores), start=1):\n",
    "        label = int(frame_labels[idx]) if idx < len(frame_labels) else -1\n",
    "        print(f'Rank {rank:02d}: frame={idx} | score={score:.4f} | label={label}')\n",
    "        ext_results.append({\n",
    "            'rank': int(rank),\n",
    "            'frame': int(idx),\n",
    "            'score': float(score),\n",
    "            'label': int(label),\n",
    "        })\n",
    "    retrieval_summary['external_query'] = {\n",
    "        'path': str(BREATH_HOLD_IMAGE_PATH),\n",
    "        'results': ext_results,\n",
    "    }\n",
    "else:\n",
    "    warning_msg = 'Breath-hold reference image is missing; external query skipped.'\n",
    "    print(warning_msg)\n",
    "    retrieval_summary['external_query'] = {\n",
    "        'path': str(BREATH_HOLD_IMAGE_PATH),\n",
    "        'warning': warning_msg,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown report generation\n",
    "from datetime import datetime\n",
    "\n",
    "if 'metrics' not in globals():\n",
    "    raise RuntimeError('Please run the evaluation cell before generating the report.')\n",
    "\n",
    "report_lines = []\n",
    "report_lines.append('# FastAP Stage Report')\n",
    "report_lines.append('')\n",
    "report_lines.append(f'- generated_at: {datetime.utcnow().isoformat()}Z')\n",
    "report_lines.append(f'- device: {DEVICE}')\n",
    "report_lines.append(f'- checkpoint_path: {CHECKPOINT_PATH}')\n",
    "report_lines.append(f'- embeddings_path: {PATHS.embeddings}')\n",
    "report_lines.append(f'- metrics_path: {PATHS.metrics}')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Config')\n",
    "report_lines.append('')\n",
    "report_lines.append(f'- backbone: {CONFIG.backbone}')\n",
    "report_lines.append(f'- projector_dim: {CONFIG.proj_dim}')\n",
    "report_lines.append(f'- embedding_dim: {CONFIG.emb_dim}')\n",
    "report_lines.append(f'- simsiam_epochs: {CONFIG.simsiam.epochs}')\n",
    "report_lines.append(f'- fastap_epochs: {CONFIG.fastap.epochs}')\n",
    "report_lines.append(f'- fastap_bins: {CONFIG.fastap.num_bins}')\n",
    "report_lines.append(f'- fastap_sigma: {CONFIG.fastap.sigma}')\n",
    "report_lines.append(f'- freeze_backbone: {CONFIG.fastap.freeze_backbone}')\n",
    "report_lines.append(f'- freeze_projector: {CONFIG.fastap.freeze_projector}')\n",
    "report_lines.append(f'- use_labels: {CONFIG.fastap.use_labels}')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## SimSiam Training History')\n",
    "if simsiam_log is None:\n",
    "    report_lines.append('SimSiam stage was skipped or not executed in this run.')\n",
    "else:\n",
    "    report_lines.append('')\n",
    "    report_lines.append(f'- epochs_trained: {len(simsiam_log.epochs)}')\n",
    "    final_loss = simsiam_log.losses[-1] if simsiam_log.losses else float('nan')\n",
    "    best_loss = min(simsiam_log.losses) if simsiam_log.losses else float('nan')\n",
    "    total_time = sum(simsiam_log.elapsed) if simsiam_log.elapsed else 0.0\n",
    "    report_lines.append(f'- final_loss: {final_loss:.4f}')\n",
    "    report_lines.append(f'- best_loss: {best_loss:.4f}')\n",
    "    report_lines.append(f'- total_time_sec: {total_time:.1f}')\n",
    "    report_lines.append('')\n",
    "    report_lines.append('| Epoch | Loss | LR | Time (s) |')\n",
    "    report_lines.append('| ----- | ---- | -- | -------- |')\n",
    "    for epoch, loss, lr, elapsed in zip(simsiam_log.epochs, simsiam_log.losses, simsiam_log.lrs, simsiam_log.elapsed):\n",
    "        report_lines.append(f'| {epoch} | {loss:.4f} | {lr:.3e} | {elapsed:.1f} |')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## FastAP Training History')\n",
    "if fastap_log is None:\n",
    "    report_lines.append('FastAP stage was skipped or not executed in this run.')\n",
    "else:\n",
    "    report_lines.append('')\n",
    "    report_lines.append(f'- epochs_trained: {len(fastap_log.epochs)}')\n",
    "    final_loss = fastap_log.losses[-1] if fastap_log.losses else float('nan')\n",
    "    best_loss = min(fastap_log.losses) if fastap_log.losses else float('nan')\n",
    "    total_time = sum(fastap_log.elapsed) if fastap_log.elapsed else 0.0\n",
    "    report_lines.append(f'- final_loss: {final_loss:.4f}')\n",
    "    report_lines.append(f'- best_loss: {best_loss:.4f}')\n",
    "    report_lines.append(f'- total_time_sec: {total_time:.1f}')\n",
    "    report_lines.append('')\n",
    "    report_lines.append('| Epoch | Loss | LR | Time (s) |')\n",
    "    report_lines.append('| ----- | ---- | -- | -------- |')\n",
    "    for epoch, loss, lr, elapsed in zip(fastap_log.epochs, fastap_log.losses, fastap_log.lrs, fastap_log.elapsed):\n",
    "        report_lines.append(f'| {epoch} | {loss:.4f} | {lr:.3e} | {elapsed:.1f} |')\n",
    "report_lines.append('')\n",
    "\n",
    "report_lines.append('## Evaluation Metrics')\n",
    "report_lines.append('')\n",
    "report_lines.append(f'- mAP: {metrics[\"mAP\"]:.4f}')\n",
    "if metrics.get('precision_at_k'):\n",
    "    report_lines.append('')\n",
    "    report_lines.append('| K | Precision | Recall |')\n",
    "    report_lines.append('| - | --------- | ------ |')\n",
    "    for k in sorted(metrics['precision_at_k'].keys()):\n",
    "        prec = metrics['precision_at_k'][k]\n",
    "        rec = metrics['recall_at_k'].get(k, float('nan'))\n",
    "        report_lines.append(f'| {k} | {prec:.4f} | {rec:.4f} |')\n",
    "report_lines.append('')\n",
    "\n",
    "if retrieval_summary:\n",
    "    report_lines.append('## Retrieval Examples')\n",
    "    report_lines.append('')\n",
    "    report_lines.append(f\"### Internal query (index={retrieval_summary.get('query_index')})\")\n",
    "    internal = retrieval_summary.get('internal', [])\n",
    "    if internal:\n",
    "        report_lines.append('| Rank | Frame | Score | Label |')\n",
    "        report_lines.append('| ---- | ----- | ----- | ----- |')\n",
    "        for row in internal:\n",
    "            report_lines.append(f\"| {row['rank']} | {row['frame']} | {row['score']:.4f} | {row['label']} |\")\n",
    "    else:\n",
    "        report_lines.append('No retrieval results were recorded.')\n",
    "    report_lines.append('')\n",
    "    external = retrieval_summary.get('external_query')\n",
    "    if external:\n",
    "        report_lines.append(f\"### External query ({external.get('path')})\")\n",
    "        if external.get('warning'):\n",
    "            report_lines.append(f\"> {external['warning']}\")\n",
    "        else:\n",
    "            report_lines.append('| Rank | Frame | Score | Label |')\n",
    "            report_lines.append('| ---- | ----- | ----- | ----- |')\n",
    "            for row in external.get('results', []):\n",
    "                report_lines.append(f\"| {row['rank']} | {row['frame']} | {row['score']:.4f} | {row['label']} |\")\n",
    "        report_lines.append('')\n",
    "\n",
    "report_text = '\\n'.join(report_lines) + '\\n'\n",
    "with open(PATHS.report, 'w', encoding='utf-8') as f:\n",
    "    f.write(report_text)\n",
    "print('Markdown report saved to', PATHS.report)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
