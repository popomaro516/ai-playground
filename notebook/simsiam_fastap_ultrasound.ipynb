{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb066b8f",
   "metadata": {},
   "source": [
    "# Ultrasound: SimSiam 学習 + FastAP 評価 (Colab Ready)\n",
    "\n",
    "このノートブックは Google Colab 上で超音波データに対する SimSiam 学習と FastAP 評価を実行するテンプレートです。\n",
    "- Google Drive から `dataset.mat` と `dataset_labels.csv` を取得し、ローカル `data/` と `annotations/` に配置します。\n",
    "- 息止め参照画像は Colab 上の `data/` (もしくはルート) に `invivo.jpg` としてアップロードしておくか、共有リンクを `SHARED_BREATH_HOLD_URL` に設定して自動ダウンロードさせます。\n",
    "- 疑似ラベルは `notebook/pseudo_labeling.ipynb` と同じ設定で生成された `annotations/dataset_labels.csv` を想定します。\n",
    "- セルを上から順に実行すれば、このノートブック単体で Colab 上でも完結するよう構成しています。\n",
    "- 実行ごとに `outputs/<timestamp>/` 以下へチェックポイント、ログ、評価指標、Top-K レポートを保存します（Colab の `files` からダウンロード可能）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494866e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (Colab などで未インストールの場合のみ実行)\n",
    "!pip install --quiet gdown pandas matplotlib h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b565abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure directories, download Google Drive assets, and set global config\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as tvm\n",
    "\n",
    "BASE_DIR = Path('.').resolve()\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "ANNOTATIONS_DIR = BASE_DIR / 'annotations'\n",
    "OUTPUTS_ROOT = BASE_DIR / 'outputs'\n",
    "for path_obj in (DATA_DIR, ANNOTATIONS_DIR, OUTPUTS_ROOT):\n",
    "    path_obj.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "RUN_DIR = OUTPUTS_ROOT / RUN_ID\n",
    "CHECKPOINTS_DIR = RUN_DIR / 'checkpoints'\n",
    "RESULTS_DIR = RUN_DIR / 'results'\n",
    "for path_obj in (RUN_DIR, CHECKPOINTS_DIR, RESULTS_DIR):\n",
    "    path_obj.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOG_PATH = RUN_DIR / 'training_log.txt'\n",
    "HISTORY_JSON_PATH = RUN_DIR / 'training_history.json'\n",
    "METRICS_JSON_PATH = RESULTS_DIR / 'metrics.json'\n",
    "TOPK_MD_PATH = RESULTS_DIR / 'topk_result.md'\n",
    "SUMMARY_PATH = RUN_DIR / 'run_summary.txt'\n",
    "print(f'Artifacts for this run will be saved under: {RUN_DIR}')\n",
    "\n",
    "SHARED_DATASET_URL = 'https://drive.google.com/file/d/1fa0gaEmbtGmqZ92L0EqzhH5LiMUAztix/view?usp=sharing'\n",
    "SHARED_BREATH_HOLD_URL = ''  # 任意: 息止め参照画像 (invivo.jpg) の共有リンク\n",
    "\n",
    "DATASET_PATH = DATA_DIR / 'dataset.mat'\n",
    "PSEUDO_LABELS_PATH = ANNOTATIONS_DIR / 'dataset_labels.csv'\n",
    "BREATH_HOLD_IMAGE_PATH = DATA_DIR / 'invivo.jpg'\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    print(f'Downloading dataset to {DATASET_PATH} ...')\n",
    "    cmd = ['gdown', '--fuzzy', SHARED_DATASET_URL, '-O', str(DATASET_PATH)]\n",
    "    result = subprocess.run(cmd, check=False)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError('Failed to download dataset from Google Drive. Please verify the shared URL.')\n",
    "else:\n",
    "    print('Dataset already present:', DATASET_PATH)\n",
    "\n",
    "if not PSEUDO_LABELS_PATH.exists():\n",
    "    alt_path = DATA_DIR / 'dataset_labels.csv'\n",
    "    if alt_path.exists():\n",
    "        alt_path.replace(PSEUDO_LABELS_PATH)\n",
    "        print('Moved pseudo labels from data/ to annotations/:', PSEUDO_LABELS_PATH)\n",
    "    else:\n",
    "        raise FileNotFoundError('annotations/dataset_labels.csv が見つかりません。pseudo_labeling ノートブックで生成するか、Drive から配置してください。')\n",
    "else:\n",
    "    print('Pseudo labels found:', PSEUDO_LABELS_PATH)\n",
    "\n",
    "if not BREATH_HOLD_IMAGE_PATH.exists():\n",
    "    moved = False\n",
    "    for candidate in (BASE_DIR / 'invivo.jpg',):\n",
    "        if candidate.exists():\n",
    "            candidate.replace(BREATH_HOLD_IMAGE_PATH)\n",
    "            print('Moved breath-hold image to data/:', BREATH_HOLD_IMAGE_PATH)\n",
    "            moved = True\n",
    "            break\n",
    "    if not moved:\n",
    "        if SHARED_BREATH_HOLD_URL:\n",
    "            print(f'Downloading breath-hold reference to {BREATH_HOLD_IMAGE_PATH} ...')\n",
    "            cmd = ['gdown', '--fuzzy', SHARED_BREATH_HOLD_URL, '-O', str(BREATH_HOLD_IMAGE_PATH)]\n",
    "            result = subprocess.run(cmd, check=False)\n",
    "            if result.returncode != 0:\n",
    "                raise RuntimeError('Failed to download breath-hold image. Please verify SHARED_BREATH_HOLD_URL.')\n",
    "        else:\n",
    "            print('Note: data/invivo.jpg が存在しません。Colab のファイルアップロードや Drive から配置してください。')\n",
    "else:\n",
    "    print('Breath-hold reference image located at', BREATH_HOLD_IMAGE_PATH)\n",
    "\n",
    "PATHS = SimpleNamespace(\n",
    "    data_dir=DATA_DIR,\n",
    "    annotations_dir=ANNOTATIONS_DIR,\n",
    "    checkpoints_dir=CHECKPOINTS_DIR,\n",
    "    dataset_mat=DATASET_PATH,\n",
    "    pseudo_labels=PSEUDO_LABELS_PATH,\n",
    "    breath_hold_image=BREATH_HOLD_IMAGE_PATH,\n",
    "    output_dir=RUN_DIR,\n",
    "    results_dir=RESULTS_DIR,\n",
    "    log_path=LOG_PATH,\n",
    "    history_json=HISTORY_JSON_PATH,\n",
    "    metrics_json=METRICS_JSON_PATH,\n",
    "    topk_markdown=TOPK_MD_PATH,\n",
    "    summary_path=SUMMARY_PATH,\n",
    "    run_id=RUN_ID,\n",
    ")\n",
    "\n",
    "SIMSIAM = SimpleNamespace(\n",
    "    seed=42,\n",
    "    image_key='Acq/Amp',\n",
    "    image_axes=(0, 2, 1),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=128,\n",
    "    epochs=10,\n",
    "    learning_rate=0.05,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    cosine_t_max=100,\n",
    "    topk=10,\n",
    "    num_workers=0,  # Colab ではマルチプロセス DataLoader がハングする場合がある\n",
    ")\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device:', DEVI\n",
    "\n",
    "random.seed(SIMSIAM.seed)\n",
    "np.random.seed(SIMSIAM.seed)\n",
    "torch.manual_seed(SIMSIAM.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SIMSIAM.seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DATA_MAT = str(PATHS.dataset_mat)\n",
    "LABEL_CSV = str(PATHS.pseudo_labels)\n",
    "IMAGE_KEY = SIMSIAM.image_key\n",
    "IMAGE_AXES = SIMSIAM.image_axes\n",
    "CHECKPOINT_DIR = str(PATHS.checkpoints_dir)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "with open(PATHS.summary_path, 'w', encoding='utf-8') as f:\n",
    "    print(f'run_id={PATHS.run_id}', file=f)\n",
    "    print(f'output_dir={PATHS.output_dir}', file=f)\n",
    "    print(f'checkpoints_dir={PATHS.checkpoints_dir}', file=f)\n",
    "    print(f'results_dir={PATHS.results_dir}', file=f)\n",
    "    print(f'dataset_mat={PATHS.dataset_mat}', file=f)\n",
    "    print(f'pseudo_labels_csv={PATHS.pseudo_labels}', file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd21963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal LazyMatImageDataset implementation (self-contained)\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "class LazyMatImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Lazily reads ultrasound frames from MATLAB v7.3 (.mat) files.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mat_files: Sequence[str],\n",
    "        image_key: str = 'Acq/Amp',\n",
    "        transform=None,\n",
    "        dtype: str = 'float32',\n",
    "        normalize_255: bool = False,\n",
    "        image_axes: Optional[Tuple[int, ...]] = None,\n",
    "    ):\n",
    "        self.mat_files = list(mat_files)\n",
    "        if not self.mat_files:\n",
    "            raise ValueError('No .mat files provided for dataset.')\n",
    "\n",
    "        self.image_key = image_key\n",
    "        self.transform = transform\n",
    "        self.dtype = dtype\n",
    "        self.normalize_255 = normalize_255\n",
    "        self._axes_override = image_axes\n",
    "\n",
    "        self._files: List[Optional[h5py.File]] = [None] * len(self.mat_files)\n",
    "        self._index: List[Tuple[int, int]] = []\n",
    "        self._image_axes: List[Tuple[int, ...]] = []\n",
    "\n",
    "        for fi, path in enumerate(self.mat_files):\n",
    "            f = h5py.File(path, 'r')\n",
    "            try:\n",
    "                ds = self._resolve_image_dataset(f, self.image_key)\n",
    "                n, c, h, w, axes = _interpret_image_shape(ds.shape, override=self._axes_override)\n",
    "                self._image_axes.append(axes)\n",
    "                self._index.extend((fi, li) for li in range(n))\n",
    "                self._files[fi] = f\n",
    "            except Exception:\n",
    "                f.close()\n",
    "                raise\n",
    "\n",
    "    def _resolve_image_dataset(self, f: h5py.File, key: str) -> h5py.Dataset:\n",
    "        if key in f:\n",
    "            return f[key]\n",
    "        for candidate in f.keys():\n",
    "            if candidate.endswith(key):\n",
    "                return f[candidate]\n",
    "        raise KeyError(f\"image_key '{key}' not found in {list(f.keys())}\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._index)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        file_idx, local_idx = self._index[idx]\n",
    "        f = self._files[file_idx]\n",
    "        if f is None:\n",
    "            f = h5py.File(self.mat_files[file_idx], 'r')\n",
    "            self._files[file_idx] = f\n",
    "\n",
    "        img_ds = self._resolve_image_dataset(f, self.image_key)\n",
    "        axes = self._image_axes[file_idx]\n",
    "        slicer = [slice(None)] * img_ds.ndim\n",
    "        slicer[axes[0]] = local_idx\n",
    "        arr = np.asarray(img_ds[tuple(slicer)])\n",
    "        arr = _ensure_chw(arr, axes)\n",
    "        if arr.dtype.kind in ('u', 'i'):\n",
    "            arr = arr.astype(self.dtype)\n",
    "            if self.normalize_255:\n",
    "                arr = arr / 255.0\n",
    "        else:\n",
    "            arr = arr.astype(self.dtype)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            return self.transform(arr)\n",
    "        return torch.from_numpy(arr)\n",
    "\n",
    "    def close(self):\n",
    "        for i, f in enumerate(self._files):\n",
    "            if f is not None:\n",
    "                f.close()\n",
    "                self._files[i] = None\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "class TwoCropsTransform:\n",
    "    \"\"\"Return two augmented views for SimSiam training.\"\"\"\n",
    "\n",
    "    def __init__(self, base_transform):\n",
    "        self.base_transform = base_transform\n",
    "\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        q = self.base_transform(x)\n",
    "        k = self.base_transform(x)\n",
    "        return q, k\n",
    "\n",
    "\n",
    "def _interpret_image_shape(\n",
    "    shape: Tuple[int, ...],\n",
    "    override: Optional[Tuple[int, ...]] = None,\n",
    ") -> Tuple[int, int, int, int, Tuple[int, ...]]:\n",
    "    if override is not None:\n",
    "        axes = tuple(override)\n",
    "        if len(axes) not in (3, 4):\n",
    "            raise ValueError('image_axes override must have length 3 or 4')\n",
    "        if len(axes) != len(shape):\n",
    "            raise ValueError('image_axes override must match dataset rank')\n",
    "        dims = [shape[a] for a in axes]\n",
    "        if len(axes) == 4:\n",
    "            n, c, h, w = dims\n",
    "            return n, c, h, w, axes\n",
    "        n, h, w = dims\n",
    "        return n, 1, h, w, axes\n",
    "\n",
    "    rank = len(shape)\n",
    "    if rank == 4:\n",
    "        candidates = [\n",
    "            (0, 3, 1, 2),\n",
    "            (0, 1, 2, 3),\n",
    "            (3, 2, 0, 1),\n",
    "            (3, 0, 1, 2),\n",
    "        ]\n",
    "        for axes in candidates:\n",
    "            n = shape[axes[0]]\n",
    "            c = shape[axes[1]]\n",
    "            h = shape[axes[2]]\n",
    "            w = shape[axes[3]]\n",
    "            if all(v > 0 for v in (n, c, h, w)):\n",
    "                return n, c, h, w, axes\n",
    "        raise ValueError(f'Unable to infer N,C,H,W from shape {shape}')\n",
    "    if rank == 3:\n",
    "        candidates = [\n",
    "            (0, 1, 2),\n",
    "            (0, 2, 1),\n",
    "            (2, 0, 1),\n",
    "            (2, 1, 0),\n",
    "            (1, 0, 2),\n",
    "            (1, 2, 0),\n",
    "        ]\n",
    "        for axes in candidates:\n",
    "            n = shape[axes[0]]\n",
    "            h = shape[axes[1]]\n",
    "            w = shape[axes[2]]\n",
    "            if all(v > 0 for v in (n, h, w)):\n",
    "                return n, 1, h, w, axes\n",
    "        raise ValueError(f'Unable to infer N,H,W from shape {shape}')\n",
    "    raise ValueError(f'Unsupported image rank {rank}; expected 3D or 4D')\n",
    "\n",
    "\n",
    "def _ensure_chw(arr: np.ndarray, axes: Tuple[int, ...]) -> np.ndarray:\n",
    "    n_axis = axes[0]\n",
    "    remaining = []\n",
    "    for a in axes[1:]:\n",
    "        remaining.append(a if a < n_axis else a - 1)\n",
    "\n",
    "    if len(remaining) == 3:\n",
    "        c_pos, h_pos, w_pos = remaining\n",
    "        return np.moveaxis(arr, (c_pos, h_pos, w_pos), (0, 1, 2))\n",
    "    if len(remaining) == 2:\n",
    "        h_pos, w_pos = remaining\n",
    "        hw = np.moveaxis(arr, (h_pos, w_pos), (0, 1))\n",
    "        return np.expand_dims(hw, axis=0)\n",
    "    raise ValueError('Unexpected axes configuration for image array')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d990ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像前処理（B-mode 変換 + 3ch化）\n",
    "def bmode_normalize(chw: np.ndarray) -> np.ndarray:\n",
    "    # chw: [C,H,W] float or double\n",
    "    x = np.abs(chw.astype(np.float32))\n",
    "    m = float(x.max())\n",
    "    x = x / (m + 1e-12)\n",
    "    x = 20.0 * np.log10(x + 1e-12)\n",
    "    x = np.clip((x + 60.0) / 60.0, 0.0, 1.0)  # [0,1]\n",
    "    return x\n",
    "\n",
    "def to_pil_3ch_from_chw01(chw01: np.ndarray) -> Image.Image:\n",
    "    # chw01: [1,H,W] or [C,H,W] in [0,1]\n",
    "    if chw01.shape[0] == 1:\n",
    "        chw01 = np.repeat(chw01, 3, axis=0)\n",
    "    hwc255 = np.transpose(chw01, (1, 2, 0))\n",
    "    hwc255 = (hwc255 * 255.0).astype(np.uint8)\n",
    "    return Image.fromarray(hwc255)\n",
    "\n",
    "# SimSiam 用の学習変換\n",
    "simsiam_aug = T.Compose([\n",
    "    # 入力: numpy [C,H,W] -> B-mode -> PIL\n",
    "    T.Lambda(lambda x: to_pil_3ch_from_chw01(bmode_normalize(x))),\n",
    "    T.RandomResizedCrop(224, scale=(0.2, 1.0), interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 評価時の変換（強いAugなし）\n",
    "eval_transform = T.Compose([\n",
    "    T.Lambda(lambda x: to_pil_3ch_from_chw01(bmode_normalize(x))),\n",
    "    T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a394fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam モデル定義（ResNet-18 バックボーン）\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim=512, hid_dim=2048, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim, bias=False),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, out_dim, bias=False),\n",
    "            nn.BatchNorm1d(out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim=2048, hid_dim=512, out_dim=2048):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hid_dim, bias=False),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimSiam(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', proj_dim=2048):\n",
    "        super().__init__()\n",
    "        # backbone\n",
    "        if backbone_name == 'resnet18':\n",
    "            backbone = tvm.resnet18(weights=tvm.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            feat_dim = 512\n",
    "        else:\n",
    "            raise ValueError('Unsupported backbone')\n",
    "        # 最終FCを除去し GlobalAvgPool 出力を使う\n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # -> [B,512,1,1]\n",
    "        self.feat_dim = feat_dim\n",
    "        self.projector = Projector(in_dim=feat_dim, hid_dim=proj_dim, out_dim=proj_dim)\n",
    "        self.predictor = Predictor(in_dim=proj_dim, hid_dim=512, out_dim=proj_dim)\n",
    "    def forward_backbone(self, x):\n",
    "        h = self.backbone(x)\n",
    "        h = torch.flatten(h, 1)  # [B, feat_dim]\n",
    "        return h\n",
    "    def forward(self, x1, x2):\n",
    "        # x1\n",
    "        h1 = self.forward_backbone(x1)\n",
    "        z1 = self.projector(h1)\n",
    "        p1 = self.predictor(z1)\n",
    "        # x2\n",
    "        h2 = self.forward_backbone(x2)\n",
    "        z2 = self.projector(h2)\n",
    "        p2 = self.predictor(z2)\n",
    "        return p1, z1, p2, z2\n",
    "\n",
    "def negative_cosine(p, z):\n",
    "    z = z.detach()\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a34c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットとローダの作成 (SimSiam 学習用)\n",
    "train_dataset = LazyMatImageDataset(\n",
    "    [DATA_MAT],\n",
    "    image_key=IMAGE_KEY,\n",
    "    normalize_255=False,\n",
    "    image_axes=IMAGE_AXES,\n",
    "    transform=TwoCropsTransform(simsiam_aug),\n",
    ")\n",
    "\n",
    "def collate_two_crops(batch):\n",
    "    # batch: list of (q,k)\n",
    "    q = torch.stack([b[0] for b in batch], dim=0)\n",
    "    k = torch.stack([b[1] for b in batch], dim=0)\n",
    "    return q, k\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=SIMSIAM.train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=SIMSIAM.num_workers,\n",
    "    pin_memory=(DEVICE == 'cuda'),\n",
    "    collate_fn=collate_two_crops,\n",
    ")\n",
    "len(train_dataset), len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55963607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 学習ループ\n",
    "import json\n",
    "\n",
    "def log_to_console_and_file(message: str) -> None:\n",
    "    '''Utility to mirror console output to the log file.'''\n",
    "    print(message, flush=True)\n",
    "    with open(PATHS.log_path, 'a', encoding='utf-8') as f:\n",
    "        print(message, file=f)\n",
    "\n",
    "run_ckpt_dir = Path(CHECKPOINT_DIR)\n",
    "run_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = SimSiam(backbone_name='resnet18', proj_dim=2048)\n",
    "save_dir = BASE_DIR / 'save'\n",
    "latest_ckpt = None\n",
    "if save_dir.exists():\n",
    "    candidates = [p for p in save_dir.glob('latest.*') if p.is_file()]\n",
    "    if candidates:\n",
    "        latest_ckpt = max(candidates, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "if latest_ckpt is not None:\n",
    "    log_to_console_and_file(f'Found pretrained weights: {latest_ckpt}')\n",
    "    try:\n",
    "        checkpoint = torch.load(latest_ckpt, map_location='cpu')\n",
    "        state_dict = checkpoint.get('state_dict', checkpoint)\n",
    "        if not isinstance(state_dict, dict):\n",
    "            raise TypeError('Checkpoint does not contain a state_dict mapping.')\n",
    "        state_dict = {k[len('module.'):] if k.startswith('module.') else k: v for k, v in state_dict.items()}\n",
    "        load_result = None\n",
    "        weights_loaded = False\n",
    "        if any(k.startswith(('backbone.', 'projector.', 'predictor.')) for k in state_dict):\n",
    "            load_result = model.load_state_dict(state_dict, strict=False)\n",
    "            matched = len(state_dict) - len(load_result.unexpected_keys)\n",
    "            weights_loaded = matched > 0\n",
    "        else:\n",
    "            backbone_state = model.backbone.state_dict()\n",
    "            target_keys = list(backbone_state.keys())\n",
    "            resnet_keys = [k for k in state_dict.keys() if not k.startswith('fc.')]\n",
    "            new_backbone_state = backbone_state.copy()\n",
    "            matched = 0\n",
    "            skipped = []\n",
    "            for tgt_key, src_key in zip(target_keys, resnet_keys):\n",
    "                tensor = state_dict[src_key]\n",
    "                if tensor.shape == new_backbone_state[tgt_key].shape:\n",
    "                    new_backbone_state[tgt_key] = tensor\n",
    "                    matched += 1\n",
    "                else:\n",
    "                    skipped.append(src_key)\n",
    "            if matched:\n",
    "                model.backbone.load_state_dict(new_backbone_state)\n",
    "                weights_loaded = True\n",
    "                log_to_console_and_file(f'Loaded {matched} backbone tensors from {latest_ckpt}.')\n",
    "                zipped_count = min(len(target_keys), len(resnet_keys))\n",
    "                unused = resnet_keys[zipped_count:] + skipped\n",
    "                if unused:\n",
    "                    log_to_console_and_file(f'Warning: skipped backbone keys (shape mismatch or excess): {unused}')\n",
    "        if load_result:\n",
    "            if load_result.missing_keys:\n",
    "                log_to_console_and_file(f'Warning: missing keys when loading weights: {load_result.missing_keys}')\n",
    "            if load_result.unexpected_keys:\n",
    "                log_to_console_and_file(f'Warning: unexpected keys when loading weights: {load_result.unexpected_keys}')\n",
    "        if weights_loaded:\n",
    "            log_to_console_and_file('Pretrained weights loaded successfully.')\n",
    "        else:\n",
    "            log_to_console_and_file('Warning: no matching parameters found in pretrained weights; proceeding with random init.')\n",
    "    except Exception as exc:\n",
    "        log_to_console_and_file(f'Failed to load weights from {latest_ckpt}: {exc}')\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=SIMSIAM.learning_rate,\n",
    "    momentum=SIMSIAM.momentum,\n",
    "    weight_decay=SIMSIAM.weight_decay,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=SIMSIAM.cosine_t_max)\n",
    "epochs = SIMSIAM.epochs  # 設定はノートブック冒頭の SIMSIAM 設定を参照\n",
    "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "scaler = torch.amp.GradScaler(device=device_type, enabled=(device_type == 'cuda'))\n",
    "\n",
    "total_batches = len(train_loader)\n",
    "if total_batches == 0:\n",
    "    raise RuntimeError('train_loader にバッチが存在しません。データの読み込み設定を確認してください。')\n",
    "log_interval = max(1, total_batches // 5)\n",
    "\n",
    "log_to_console_and_file(f'Start training for {epochs} epochs ({total_batches} batches/epoch).')\n",
    "\n",
    "history = []\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    log_to_console_and_file(f'-- Epoch {epoch + 1}/{epochs} --')\n",
    "    for batch_idx, (x1, x2) in enumerate(train_loader, start=1):\n",
    "        x1 = x1.to(DEVICE, non_blocking=True)\n",
    "        x2 = x2.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=device_type, enabled=(device_type == 'cuda')):\n",
    "            p1, z1, p2, z2 = model(x1, x2)\n",
    "            loss = 0.5 * negative_cosine(p1, z2) + 0.5 * negative_cosine(p2, z1)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += float(loss.item())\n",
    "\n",
    "        if batch_idx % log_interval == 0 or batch_idx == total_batches:\n",
    "            elapsed = time.time() - t0\n",
    "            avg_so_far = epoch_loss / batch_idx\n",
    "            lr_now = optimizer.param_groups[0]['lr']\n",
    "            log_to_console_and_file(\n",
    "                f'  batch {batch_idx}/{total_batches} '\n",
    "                f'loss={loss.item():.4f} avg_loss={avg_so_far:.4f} '\n",
    "                f'lr={lr_now:.6f} elapsed={elapsed:.1f}s'\n",
    "            )\n",
    "    scheduler.step()\n",
    "    dt = time.time() - t0\n",
    "    avg_loss = epoch_loss / max(1, total_batches)\n",
    "    msg = f'Epoch {epoch+1}/{epochs} loss={avg_loss:.4f} time={dt:.1f}s'\n",
    "    log_to_console_and_file(msg)\n",
    "    history.append({'epoch': epoch + 1, 'loss': avg_loss, 'time_sec': dt})\n",
    "\n",
    "    latest_path = run_ckpt_dir / 'simsiam_latest.pth'\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()}, latest_path)\n",
    "    epoch_path = run_ckpt_dir / f'simsiam_epoch_{epoch + 1:02d}.pth'\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict()}, epoch_path)\n",
    "    log_to_console_and_file(f'Checkpoint updated: {latest_path}')\n",
    "\n",
    "with open(PATHS.history_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "if history:\n",
    "    with open(PATHS.summary_path, 'a', encoding='utf-8') as f:\n",
    "        print(f'epochs_completed={len(history)}', file=f)\n",
    "        print(f'final_loss={history[-1][\"loss\"]:.4f}', file=f)\n",
    "        print(f'checkpoint_latest={latest_path}', file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込み抽出（評価用）\n",
    "from pathlib import Path\n",
    "eval_dataset = LazyMatImageDataset(\n",
    "    [DATA_MAT], image_key=IMAGE_KEY, normalize_255=False, image_axes=IMAGE_AXES, transform=eval_transform\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "    eval_dataset, batch_size=SIMSIAM.eval_batch_size, shuffle=False,\n",
    "    num_workers=SIMSIAM.num_workers, pin_memory=(DEVICE == 'cuda')\n",
    ")\n",
    "\n",
    "def extract_embeddings(model: SimSiam, loader: DataLoader) -> np.ndarray:\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            h = model.forward_backbone(xb)\n",
    "            z = model.projector(h)\n",
    "            z = F.normalize(z, dim=1)\n",
    "            embs.append(z.cpu().numpy())\n",
    "    return np.concatenate(embs, axis=0)\n",
    "\n",
    "embeddings = extract_embeddings(model, eval_loader)\n",
    "embeddings_path = Path(CHECKPOINT_DIR) / 'simsiam_embeddings.npy'\n",
    "np.save(embeddings_path, embeddings)\n",
    "print(f'Embeddings saved to {embeddings_path}')\n",
    "with open(PATHS.summary_path, 'a', encoding='utf-8') as f:\n",
    "    print(f'embeddings_path={embeddings_path}', file=f)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09198311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAP 相当のランキング評価 (AP/mAP) を疑似ラベルで評価\n",
    "import json\n",
    "labels_df = pd.read_csv(LABEL_CSV)\n",
    "labels = labels_df.set_index('frame').loc[range(len(eval_dataset)), 'label'].to_numpy()\n",
    "labels = labels.astype(int)\n",
    "\n",
    "def average_precision_for_query(sim_vec: np.ndarray, rel: np.ndarray) -> float:\n",
    "    # sim_vec: (N,) 類似度（自分自身は含めないこと）\n",
    "    # rel: (N,) 0/1 relevance\n",
    "    order = np.argsort(-sim_vec)\n",
    "    rel_sorted = rel[order]\n",
    "    n_rel = int(rel_sorted.sum())\n",
    "    if n_rel == 0:\n",
    "        return 0.0\n",
    "    cumsum = np.cumsum(rel_sorted)\n",
    "    idx = np.arange(1, len(rel_sorted) + 1)\n",
    "    prec_at_k = (cumsum / idx) * rel_sorted\n",
    "    return float(prec_at_k.sum() / n_rel)\n",
    "\n",
    "def compute_map(embs: np.ndarray, labels: np.ndarray) -> float:\n",
    "    # cosine similarity (embeddings are normalized)\n",
    "    sim_all = embs @ embs.T\n",
    "    n = sim_all.shape[0]\n",
    "    ap_list = []\n",
    "    for i in range(n):\n",
    "        sim_i = sim_all[i].copy()\n",
    "        rel_i = (labels == labels[i]).astype(np.int32)\n",
    "        # 自分自身を除外\n",
    "        sim_i[i] = -np.inf\n",
    "        rel_i[i] = 0\n",
    "        ap = average_precision_for_query(sim_i, rel_i)\n",
    "        ap_list.append(ap)\n",
    "    return float(np.mean(ap_list)), ap_list\n",
    "\n",
    "mAP, ap_list = compute_map(embeddings, labels)\n",
    "metrics = {\n",
    "    'mAP': mAP,\n",
    "    'num_frames': len(eval_dataset),\n",
    "    'num_positive_labels': int(labels.sum()),\n",
    "    'timestamp': time.time(),\n",
    "}\n",
    "with open(PATHS.metrics_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f'mAP (pseudo labels): {mAP:.4f} -> saved to {PATHS.metrics_json}')\n",
    "with open(PATHS.summary_path, 'a', encoding='utf-8') as f:\n",
    "    print(f'mAP={mAP:.4f}', file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26193cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 息止め画像を与えて、類似フレーム Top-K を返す評価\n",
    "from pathlib import Path\n",
    "BREATH_HOLD_IMAGE = str(PATHS.breath_hold_image)\n",
    "TOPK = SIMSIAM.topk\n",
    "\n",
    "def preprocess_external_image(path: str) -> torch.Tensor:\n",
    "    pil = Image.open(path).convert('RGB')\n",
    "    t = T.Compose([\n",
    "        T.Resize(256, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])(pil)\n",
    "    return t\n",
    "\n",
    "def embed_image_tensor(model: SimSiam, img_t: torch.Tensor) -> np.ndarray:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xb = img_t.unsqueeze(0).to(DEVICE)\n",
    "        h = model.forward_backbone(xb)\n",
    "        z = model.projector(h)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return z.cpu().numpy()[0]\n",
    "\n",
    "path_obj = Path(BREATH_HOLD_IMAGE)\n",
    "if path_obj.exists():\n",
    "    q = preprocess_external_image(BREATH_HOLD_IMAGE)\n",
    "    q_emb = embed_image_tensor(model, q)\n",
    "    sims = embeddings @ q_emb\n",
    "    top_idx = np.argsort(-sims)[:TOPK]\n",
    "    lines = [\n",
    "        f'# Top-{TOPK} Similar Frames',\n",
    "        f'- Query image: {path_obj}',\n",
    "        f'- Embedding source: {Path(CHECKPOINT_DIR) / \"simsiam_embeddings.npy\"}',\n",
    "        '',\n",
    "        '| Rank | Frame | Cosine sim | Label |',\n",
    "        '| ---- | ----- | ---------- | ----- |',\n",
    "    ]\n",
    "    print(f'Top-{TOPK} 類似フレーム:')\n",
    "    for rank, idx in enumerate(top_idx, 1):\n",
    "        line = f'{rank:2d}: frame={idx} sim={sims[idx]:.4f} label={labels[idx]}'\n",
    "        print(line)\n",
    "        lines.append(f'| {rank} | {idx} | {sims[idx]:.4f} | {labels[idx]} |')\n",
    "    report_text = '\\n'.join(lines) + '\\n'\n",
    "    with open(PATHS.topk_markdown, 'w', encoding='utf-8') as f:\n",
    "        f.write(report_text)\n",
    "    print(f'Top-{TOPK} report saved to {PATHS.topk_markdown}')\n",
    "    with open(PATHS.summary_path, 'a', encoding='utf-8') as f:\n",
    "        print(f'topk_report={PATHS.topk_markdown}', file=f)\n",
    "else:\n",
    "    warning_msg = 'data/invivo.jpg が見つかりません。Colab にアップロードするか、SHARED_BREATH_HOLD_URL を設定して再実行してください。'\n",
    "    print(warning_msg)\n",
    "    with open(PATHS.summary_path, 'a', encoding='utf-8') as f:\n",
    "        print('topk_report=missing_input', file=f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
